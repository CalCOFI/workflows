---
title: "Ingest NOAA CalCOFI Database"
execute:
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console
format:
  html:
    code-fold: true  
editor: 
  markdown: 
    wrap: 72
---

## Overview {.unnumbered}

**Goal**: Generate the database from source files with workflow scripts
to make updating easier and provenance fully transparent. This allows us
to:

-   Rename tables and column names, control data types and use Unicode
    encoding for a consistent database ingestion strategy, per Database
    naming conventions in [Database – CalCOFI.io
    Docs](https://calcofi.io/docs/db.html).

-   Differentiate between raw and derived or updated tables and columns.
    For instance, the taxonomy for any given species can change over
    time, such as lumping or splitting of a given taxa, and by taxonomic
    authority (e.g., WoRMS, ITIS or GBIF). These taxonomic identifiers
    and the full taxonomic hierarchy should get regularly updated
    regardless of source observational data, and can either be updated
    in the table directly or joined one-to-one with a seperate table in
    a materialized view (so as not to slow down queries with a regular
    view).

This workflow processes NOAA CalCOFI database CSV files and updates the
PostgreSQL database. The workflow:

1.  Reads CSV files from source directory
2.  Compares with lookup tables for field descriptions
3.  Initializes or updates database tables
4.  Generates summary statistics

```{mermaid}
%%| label: overview
%%| fig-cap: "Overview diagram of CSV ingestion process into the database."
%%| file: diagrams/ingest_noaa-calcofi-db_overview.mmd
```

```{r}
#| label: setup

librarian::shelf(
  DBI, dm, DT, glue, here, janitor, lubridate, purrr, readr, rlang, 
  tibble, tidyr, uuid,
  quiet = T)
options(readr.show_col_types = F)

# Source database connection
source(here("../apps_dev/libs/db.R"))

# define paths ---

# dataset with external data folder
dir_data <- "/Users/bbest/My Drive/projects/calcofi/data"
dataset  <- "noaa-calcofi-db"
dir_csv  <- glue("{dir_data}/{dataset}")
# input tables (version controlled)
tbls_in_csv <- here(glue("data/ingest/{dataset}/tbls_in.csv"))
flds_in_csv <- here(glue("data/ingest/{dataset}/flds_in.csv"))
# rename tables (version controlled)
tbls_rn_csv <- here(glue("data/ingest/{dataset}/tbls_rename.csv"))
flds_rn_csv <- here(glue("data/ingest/{dataset}/flds_rename.csv"))
```

## Read CSV Files

```{r}
#| label: read_csvs

# read data, extract field headers
d <- tibble(
  csv = list.files(
    dir_csv, pattern = "\\.csv$", full.names = TRUE)) |> 
  mutate(
    tbl  = tools::file_path_sans_ext(basename(csv)),
    data = map(csv, read_csv),
    nrow = map_int(data, nrow),
    ncol = map_int(data, ncol),
    flds = map2(tbl, data, \(tbl, data){
      tibble(
        fld  = names(data),
        type = map_chr(fld, \(fld) class(data[[fld]])[1] )) })) |> 
  relocate(tbl)

if (!dir.exists(dirname(tbls_in_csv)))
  dir.create(dirname(tbls_in_csv), recursive = T)

d_tbls_in <- d |> 
  select(tbl, nrow, ncol)
write_csv(d_tbls_in, tbls_in_csv)

d_flds_in <- d |> 
  select(tbl, flds) |> 
  unnest(flds) 
write_csv(d_flds_in, flds_in_csv)

datatable(d_tbls_in, caption = "Tables to ingest.")
```

```{r}
#| label: dt_flds_in

datatable(d_flds_in, caption = "Fields to ingest.")
```

## Show table and field renames

```{r}
#| label: create_or_read_flds_rn_csv

if (!file.exists(flds_rn_csv)){
  d_tbls_in |> 
    select(
      tbl_old = tbl, 
      tbl_new = tbl) |> 
    write_csv(tbls_rn_csv)

  d_flds_in |> 
    group_by(tbl) |> 
    mutate(
      fld_new  = make_clean_names(fld),
      order    = 1:n(),
      comment  = "",
      notes    = "",
      mutation = "",
      type_new = map2_chr(tbl, fld, \(x, y){
        
        v <- d |> 
          filter(
            tbl == x) |> 
          pull(data) |> 
          pluck(1) |> 
          pull(y)
        
        cl <- class(v)[1]
        
        if (cl == "character"){
          if (all(!is.na(as.UUID(v))))
            return("uuid")
        } else {
          return("varchar")
        }
        
        # integer
        if (cl == "numeric" && all(v%%1==0)){
          # https://www.postgresql.org/docs/current/datatype-numeric.html
          if (
            min(v) >= -32768 & 
            max(v) <= 32767)
            return("smallint")
          if (
            min(v) >= -2147483648 & 
            max(v) <= 2147483647)
            return("integer")
          if (
            min(v) >= -9223372036854775808 & 
            max(v) <= 9223372036854775807)
            return("bigint")
        }
        
        if (cl == "POSIXct")
          return("timestamp")
        if (cl == "Date")
          return("date")
        
        return(cl)
        })) |> 
    select(
      tbl_old   = tbl,   tbl_new = tbl,
      fld_old   = fld,   fld_new,
      order_old = order, order_new = order,
      type_old  = type,  type_new,
      comment, notes, mutation) |> 
    write_csv(flds_rn_csv)
  
  stop(glue(
    "Please update the table and field rename tables before proceeding:
      {tbls_rn_csv}
      {flds_rn_csv}"))
}

d_tbls_rn <- read_csv(tbls_rn_csv)
d_flds_rn <- read_csv(flds_rn_csv)

d_tbls_rn |>
  mutate(
    is_equal = tbl_old == tbl_new) |> 
  datatable(
    caption = "Tables to rename.",
    options = list(
      columnDefs = list(list(
        targets = "is_equal", visible = F)))) |> 
  formatStyle(
    "tbl_new",
    backgroundColor = styleEqual(
      c(T,F), 
      c("lightgray","lightgreen")),
    valueColumns    = "is_equal")
```

```{r}
#| label: dt_flds_rn

d_flds_rn |>
  mutate(
    tbl_is_equal   = tbl_old   == tbl_new,
    fld_is_equal   = fld_old   == fld_new,
    type_is_equal  = type_old  == type_new,
    order_is_equal = order_old == order_new,
    tbl = ifelse(
      tbl_is_equal,
      tbl_old,
      glue("{tbl_old} → {tbl_new}")),
    fld = ifelse(
      fld_is_equal,
      fld_old,
      glue("{fld_old} → {fld_new}")),
    type = ifelse(
      type_is_equal,
      type_old,
      glue("{type_old} → {type_new}")),
    order = ifelse(
      order_is_equal,
      order_old,
      glue("{order_old} → {order_new}"))) |> 
  select(
    -tbl_old,   -tbl_new,
    -fld_old,   -fld_new,
    -type_old,  -type_new,
    -order_old, -order_new) |> 
  relocate(tbl, fld, type, order) |> 
  datatable(
    caption = "Fields to rename.",
    rownames = F,
    options = list(
      colReorder = T,
      rowGroup = list(dataSrc = 0),
      pageLength = 50,
      columnDefs = list(list(
        targets = c(
          "tbl",
          "tbl_is_equal", "fld_is_equal", 
          "type_is_equal", "order_is_equal"), 
        visible = F))),
    extensions = c("ColReorder", "RowGroup", "Responsive")) |> 
  formatStyle(
    "fld",
    backgroundColor = styleEqual(
      c(T, F), 
      c("lightgray","lightgreen")),
    valueColumns    = "fld_is_equal") |> 
  formatStyle(
    "type",
    backgroundColor = styleEqual(
      c(T, F), 
      c("lightgray","lightgreen")),
    valueColumns    = "type_is_equal") |> 
  formatStyle(
    "order",
    backgroundColor = styleEqual(
      c(T, F), 
      c("lightgray","lightgreen")),
    valueColumns    = "order_is_equal")
```

## Apply remappings to data

```{r}
#| label: make_data_new

d0 <- d
mutate_table <- function(tbl, data) {
  # message(glue("Processing table: {tbl}"))
  
  d_f <- d_flds_rn |>
    filter(tbl_old == tbl)
  
  # rename fields ----
  f_rn <- d_f |>
    select(fld_old, fld_new) |> 
    deframe()
  
  y <- rename_with(data, ~ f_rn[.x])
  
  # mutate fields ----
  d_m <- d_f |> 
    select(fld_new, mutation) |> 
    filter(!is.na(mutation))
  
  for (i in seq_len(nrow(d_m))) {
    fld <- d_m$fld_new[i]
    mx  <- d_m$mutation[i]
    
    # message(glue("mutating {tbl}.{fld}: `{mx}`"))
    fld_sym <- rlang::sym(fld)
    mx_expr <- rlang::parse_expr(mx)
    
    y <- y |>
      mutate(!!fld_sym := eval(mx_expr, envir = y))
  }
  
  # order fields ----
  flds_ordered <- d_f |>
    arrange(order_new) |> 
    pull(fld_new)
  y <- y |>
    relocate(all_of(flds_ordered))

  return(y)
}

d <- d |>
  left_join(
    d_tbls_rn,
    by = c("tbl" = "tbl_old")) |> 
  mutate(
    data_new = map2(tbl, data, mutate_table))
```

## Load Tables into Database

```{r}
#| label: load_tbls_to_db

schema    <- "dev"
overwrite <- T

tbl_to_db <- function(tbl){ # tbl = "egg" # tbl = "ship"
  message(glue("{schema}.{tbl}  ~ {Sys.time()}" ))

  # Check if table exists
  tbl_exists <- dbExistsTable(
    con, Id(schema = schema, table = tbl))
  
  d_tbl <- d |> 
    filter(tbl_new == !!tbl) |> 
    pull(data_new) |> 
    pluck(1)
  
  v_fld_types <- d_flds_rn |> 
    filter(tbl_new == !!tbl) |> 
    arrange(order_new) |> 
    select(fld_new, type_new) |> 
    deframe()
  
  if (!tbl_exists | overwrite) {
    
    message("  loading table")
    dbWriteTable(
      con, 
      Id(schema = schema, table = tbl), 
      d_tbl, 
      field.types = v_fld_types, 
      append      = F,
      overwrite   = T)
    
  } else {
    
    message("  exists, skipping")
    
    # # Get existing data
    # existing_data <- dbGetQuery(
    #   con, sprintf("SELECT * FROM %s LIMIT 1", table_name))
    # 
    # # Compare schemas
    # new_cols <- setdiff(names(data), names(existing_data))
    # if (length(new_cols) > 0) {
    #   cat(sprintf("New columns found in %s: %s\n", table_name, paste(new_cols, collapse = ", ")))
    # }
    # 
    # # Update data - this is a simple example, you might want more sophisticated merge logic
    # dbWriteTable(con, table_name, data, overwrite = TRUE)
  }
  
  # Return summary stats
  return(tibble(
    tbl = tbl,
    nrow = nrow(d_tbl),
    ncol = ncol(d_tbl),
    dtime_ingested = Sys.time()))
}

# Process each table
tbl_stats <- map(d$tbl_new, tbl_to_db) |> 
  bind_rows()

# Display summary statistics
tbl_stats |> 
  datatable(rownames = FALSE, filter = "top")
```

## Create Indexes, Relationships

```{r}
#| label: dm_tbls

# source(here("../apps_dev/libs/db.R"))

tbls_dev <- dbListTables(con_dev) |> sort()

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")
```

### Show existing unique keys across tables

```{r}
#| label: show_keys

dm_get_all_uks(dm_dev)
```

### Add candidate keys, automatically

```{r}
#| label: mk_keys_auto

# Function to get primary key constraints for a table
tbl_pkeys <- function(con, tbl) {
  dbGetQuery(con, glue("
    SELECT a.attname as column_name
    FROM pg_index i
    JOIN pg_attribute a ON a.attrelid = i.indrelid
    AND a.attnum = ANY(i.indkey)
    WHERE i.indrelid = '{tbl}'::regclass
    AND i.indisprimary;
  "))$column_name
}

d_pk <- tibble(
  tbl = tbls_dev) |> 
  mutate(
    d        = map(tbl, \(x) eval(bquote(dm_enum_pk_candidates(dm_dev, .(x)))) ),
    n        = map_int(d, \(x) x |> filter(candidate == T) |> nrow()),
    flds     = map_chr(d, \(x){
      x |> filter(candidate == T) |> pull(columns) |> 
        unlist() |> paste(collapse = "; ") }),
    fld      = map_chr(d, \(x){
      y <- x |> filter(candidate == T) |> pull(columns) |> 
        unlist()
      if (length(y) == 0)
        return(NA)
      if (length(y) > 1)
        return(y[1])
      y }),
    add_pkey = map2_lgl(tbl, fld, \(tbl, fld){
      if (is.na(fld))
        return(F)
      # tbl <- "cruise"; fld = "cruise_uuid"
      pkey_exists <- tbl_pkeys(con_dev, "cruise") |> length() > 0
      if (pkey_exists)
        return(F)
      dbExecute(con_dev, glue("ALTER TABLE {tbl} ADD PRIMARY KEY ({fld})"))
      return(T) }) ) |> 
  select(-d)
    
d_pk |> 
  datatable()
```

### Add primary keys, manually

Noticing egg and larva missing pkey.

```{r}
#| label: mk_keys_manual

dbExecute(con_dev, "ALTER TABLE egg ADD PRIMARY KEY (net_uuid, sp_id)")
dbExecute(con_dev, "ALTER TABLE larva ADD PRIMARY KEY (net_uuid, sp_id)")

tibble(
  tbl = tbls_dev) |> 
  mutate(
    pkeys = map_chr(tbl, \(x) tbl_pkeys(x, con = con_dev) |> paste(collapse = ", "))) |> 
  datatable()
```

### Add other indexes

That are not already the first field of a primary key.

```{r}
#| label: mk_idx

d_idx <- tibble(
  tbl = tbls_dev) |>
  mutate(
    fld   = map(tbl, ~dbListFields(.x, conn = con_dev)),
    pkeys = map(tbl, ~tbl_pkeys(.x, con = con_dev))) |> 
  unnest(fld) |>
  mutate(
    is_pkey  = map2_lgl(fld, pkeys, `%in%`),
    is_pkey1 = map2_lgl(fld, pkeys, \(fld, pkeys) fld == pkeys[1]),
    is_id    = map_lgl(fld, ~str_detect(.x, "_(id|uuid|tsn)$")),
    idx_todo = !is_pkey1 & is_id,
    mk_idx   = pmap_lgl(list(tbl, fld, idx_todo), \(tbl, fld, idx_todo){
      if (idx_todo){
        q <- glue("CREATE INDEX IF NOT EXISTS idx_{tbl}_{fld} ON {tbl} ({fld})")
        # message(q)
        dbExecute(con_dev, q)
        return(T)
      }
      return(F)
    }))

d_idx |> 
  select(-pkeys) |> 
  datatable()
```

### Add foreign keys

egg.net_uuid -> net.net_uuid .tow_uuid -> tow.tow_uuid .site_uuid -> site.site_uuid .cruise_uuid -> cruise.cruise_uuid .ship_key -> ship.ship_key

```{r}
#| label: dm_fk

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")

dm_dev_fk <- dm_dev |> 
  dm_add_fk(egg, net_uuid, net) |> 
  dm_add_fk(larva, net_uuid, net) |> 
  dm_add_fk(net, tow_uuid, tow) |> 
  dm_add_fk(tow, site_uuid, site) |> 
  dm_add_fk(site, cruise_uuid, cruise) |> 
  dm_add_fk(cruise, ship_key, ship) |> 
  dm_add_fk(tow, tow_type_key, tow_type) |> 
  dm_add_fk(egg, sp_id, species) |> 
  dm_add_fk(larva, sp_id, species) 
dm_draw(dm_dev_fk, view_type = "all")
```

```{r}
#| label: sql_fk

add_fkey <- function(tbl_m, fld_m, tbl_1, fld_1 = fld_m, schema = "dev"){
  q <- glue(
    "ALTER TABLE {schema}.{tbl_m} ADD FOREIGN KEY ({fld_m}) 
    REFERENCES {schema}.{tbl_1} ({fld_1})")
  dbExecute(con_dev, q)
}

add_fkey("egg", "net_uuid", "net")
add_fkey("larva", "net_uuid", "net") 
add_fkey("net", "tow_uuid", "tow") 
add_fkey("tow", "site_uuid", "site")
add_fkey("site", "cruise_uuid", "cruise")
add_fkey("cruise", "ship_key", "ship")
add_fkey("tow", "tow_type_key", "tow_type")
add_fkey("egg", "sp_id", "species")
add_fkey("larva", "sp_id", "species")

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")
```

```
Error: Failed to fetch row : ERROR:  insert or update on table "larva" violates foreign key constraint "larva_sp_id_fkey"
DETAIL:  Key (sp_id)=(3023) is not present in table "species".
```

```{r}
#| label: sp_missing

sp_missing_csv <- here(
    "data/ingest/noaa-calcofi-db/larvae-species_not-in-species.csv")

d_sp_missing <- tbl(con_dev, "larva") |> 
  anti_join(
    tbl(con_dev, "species"),
    by = "sp_id") |> 
  group_by(sp_id) |> 
  summarize(
    n_rows = n()) |> 
  left_join(
    dm_dev |>
      dm_select_tbl(larva, net, tow, site) |> 
      dm_flatten_to_tbl(.start = larva, .recursive = TRUE) |> 
      select(sp_id, tally, time_start, line, station, cruise_uuid) |> 
      group_by(sp_id) |> 
      summarize(
        sum_tally  = sum(tally),
        date_beg   = min(as.Date(time_start)),
        date_end   = max(as.Date(time_start)),
        # ship_names = str_flatten(distinct(name), collapse = "; ")), 
        n_cruises = n_distinct(cruise_uuid)), 
    by = "sp_id") |> 
  collect()

write_csv(d_sp_missing, sp_missing_csv)

datatable(d_sp_missing)
```


```{r}
#| label: delete_larva_sp_missing

dbExecute(con_dev, glue(
  "DELETE FROM larva WHERE sp_id IN ({paste(d_sp_missing$sp_id, collapse = ',')})"))
add_fkey("larva", "sp_id", "species")

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")
```

## Add Spatial

### Add `site.geom`

```{r}
#| label: mk_site_pts

# source(here("../apps_dev/libs/db.R"))
# dbGetQuery(con_dev, "SELECT postgis_full_version()")

dbExecute(con_dev, "ALTER TABLE dev.site ADD COLUMN geom geometry(Point, 4326)")
dbExecute(con_dev, "UPDATE dev.site SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)") # 61,220 rows
```

### Fix calcofi4r `grid`

Problems with [calcofi4r::`cc_grid`](https://calcofi.io/calcofi4r/articles/calcofi4r.html):

- uses old station (line, position) vs newer site (line, station)
- `sta_lin`, `sta_pos`: integer, so drops necessary decimal that is found in `sta_key`
- `sta_lin == 90, sta_pos == 120` repeats for:
  * `sta_pattern` == 'historical' (`sta_dpos` == 20); and 
  * `sta_pattern` == 'standard'   (`sta_dpos` == 10)

```{r}
#| label: mk_grid_v2

librarian::shelf(
  calcofi4r, dplyr, mapview, sf, tidyr, units)

cc_grid_v2 <- calcofi4r::cc_grid |> 
  select(sta_key, shore = sta_shore, pattern = sta_pattern, spacing = sta_dpos) |>
  separate_wider_delim(
    sta_key, ",", names = c("line", "station"), cols_remove = F) |> 
  mutate(
    line    = as.double(line),
    station = as.double(station),
    grid_key = ifelse(
      pattern == "historical", 
      glue("st{station}-ln{line}_hist"),
      glue("st{station}-ln{line}")),
    zone = glue("{shore}-{pattern}")) |> 
  relocate(grid_key, station) |> 
  st_as_sf() |> 
  mutate(
    area_km2 = st_area(geom) |>
      set_units(km^2) |>
      as.numeric())

cc_grid_ctrs_v2 <- calcofi4r::cc_grid_ctrs |> 
  select(sta_key, pattern = sta_pattern) |> 
  left_join(
    cc_grid_v2 |> 
      st_drop_geometry(),
    by = c("sta_key", "pattern")) |> 
  select(-sta_key) |> 
  relocate(grid_key)
  
cc_grid_v2 <- cc_grid_v2 |> 
  select(-sta_key)

cc_grid_v2 |> 
  st_drop_geometry() |> 
  datatable()

mapview(cc_grid_v2, zcol="zone") +
  mapview(cc_grid_ctrs_v2, cex = 1)
```

```{r}
#| label: grid_to_db

grid <- cc_grid_v2 |> 
  as.data.frame() |> 
  left_join(
    cc_grid_ctrs_v2 |> 
      as.data.frame() |> 
      select(grid_key, geom_ctr = geom),
    by = "grid_key") |> 
  st_as_sf(sf_column_name = "geom")

grid |> 
  st_write(con_dev, "grid")
dbExecute(con_dev, glue(
  "CREATE INDEX IF NOT EXISTS idx_grid_geom ON grid USING gist(geom);"))
dbExecute(con_dev, "ALTER TABLE grid ADD PRIMARY KEY (grid_key)")
```

### Update `site.grid_key`

```{r}
#| label: update_site_from_grid

dbExecute(con_dev, "ALTER TABLE site ADD COLUMN grid_key text")

dbExecute(con_dev, "
  UPDATE site
  SET
    grid_key = g.grid_key
  FROM (
    SELECT s.site_uuid, g.grid_key
    FROM site s
    INNER JOIN grid g ON ST_Intersects(s.geom, g.geom)) g
  WHERE site.site_uuid = g.site_uuid") # 59,136

# tbl(con_dev, "site") |>
#   mutate(is_in_grd = !is.na(grid_key)) |>
#   pull(is_in_grd) |>
#   table()
#  FALSE    TRUE 
#  2,084  59,136

dbExecute(con_dev, glue(
  "CREATE INDEX IF NOT EXISTS idx_site_geom_key ON grid (grid_key);"))
add_fkey("site", "grid_key", "grid")

dm_from_con(
  con_dev, 
  # table_names = dbListTables(con_dev_only),
  table_names = c("site", "grid"),
  learn_keys  = T) |> 
  dm_set_colors(green = grid) |>
  dm_draw(view_type = "all")
```

### Add `site_seg`: segments from site

```{r}
#| label: mk_site_seg

site_seg <- tbl(con_dev, "site") |> 
  select(cruise_uuid, orderocc, site_uuid, lon = longitude, lat = latitude) |> 
  left_join(
    tbl(con_dev, "tow") |> 
      select(site_uuid, time_start),
    by = "site_uuid") |>
  arrange(cruise_uuid, orderocc) |> 
  group_by(
    cruise_uuid, orderocc, site_uuid, lon, lat) |> 
  summarize(
    time_beg = min(time_start, na.rm = T),
    time_end = max(time_start, na.rm = T),
    .groups = "drop") |> 
  collect() |> 
# table(is.na(site_seg$time_beg))
#   FALSE   TRUE 
#  57,935  3,285
site_seg <- site_seg |>   
  arrange(cruise_uuid, orderocc, time_beg) |> 
  group_by(cruise_uuid) |> 
  mutate(
    site_uuid_beg = lag(site_uuid),
    lon_beg       = lag(lon),
    lat_beg       = lag(lat),
    time_beg      = lag(time_beg)) |> 
  ungroup() |> 
  filter(!is.na(lon_beg), !is.na(lat_beg)) |> 
  mutate(
    m    = pmap(
      list(lon_beg, lat_beg, lon, lat), 
      \(x1, y1, x2, y2){
        matrix(c(x1, y1, x2, y2), nrow = 2, byrow = T) }),
    geom = map(m, st_linestring)) |> 
  select(
    cruise_uuid,
    site_uuid_beg, 
    site_uuid_end = site_uuid, 
    lon_beg, 
    lat_beg,
    lon_end = lon, 
    lat_end = lat,
    time_beg, 
    time_end, 
    geom) |> 
  st_as_sf(
    sf_column_name = "geom", 
    crs = 4326) |> 
  mutate(
    time_hr   = as.numeric(difftime(time_end, time_beg, units = "hours")),
    length_km = st_length(geom) |>
      set_units(km) |>
      as.numeric(),
    km_per_hr = length_km / time_hr)

# TODO: check that time_end of previous segment is time_beg of next

# TODO: investigate too slow, too fast
# site_seg |> 
#   mutate(
#     km_per_hr_flag = ifelse(
#       km_per_hr < 0.01,
#       "lt0.01",
#       ifelse(
#         km_per_hr > 30,
#         "gt30",
#         NA))) |> 
#   pull(km_per_hr_flag) |> 
#   table()
#   gt30 lt0.01 
#    928      9

fld_types <- tibble(
  lst = lapply(site_seg, class)) |> 
  mutate(
    fld    = names(lst),
    type_r = map_chr(lst, pluck, 1),
    type   = map2_chr(fld, type_r, \(fld, type_r){
      case_when(
        str_detect(fld, "uuid") ~ "uuid",
        fld    == "geom"        ~ "geometry",
        type_r == "POSIXct"     ~ "timestamp",
        type_r == "character"   ~ "varchar",
        .default = "numeric") })) |> 
  select(fld, type) |> 
  deframe()
st_write(site_seg, con_dev, "site_seg", field.types = fld_types)
dbExecute(con_dev, "CREATE INDEX IF NOT EXISTS idx_site_seg_geom ON grid USING gist(geom)")
dbExecute(con_dev, "ALTER TABLE site_seg ADD PRIMARY KEY (site_uuid_beg)")
dbExecute(con_dev, 'CREATE EXTENSION IF NOT EXISTS "uuid-ossp"')
add_fkey("site_seg", "site_uuid_beg", "site", "site_uuid")
add_fkey("site_seg", "site_uuid_end", "site", "site_uuid")
add_fkey("site_seg", "cruise_uuid", "cruise")

dm_from_con(
  con_dev, 
  # table_names = dbListTables(con_dev_only),
  table_names = c("site_seg", "site", "cruise"),
  learn_keys  = T) |> 
  dm_set_colors(green = site_seg) |>
  dm_draw(view_type = "all")

site_seg <- st_read(con_dev, "site_seg") |> 
  mutate(
    year = year(time_beg))

mapView(site_seg, zcol = "year")
```

## Report

```{r}
#| label: show_latest

dm_from_con(
  con_dev, 
  table_names = dbListTables(con_dev_only),
  # table_names = c("site_seg", "site", "cruise"),
  learn_keys  = T) |> 
  dm_set_colors(lightgreen = c(site_seg, grid)) |>
  dm_draw(view_type = "all")

d_eff <- tbl(con_dev, "site_seg") |> 
  mutate(
    year = year(time_beg)) |> 
  group_by(year) |> 
  summarize(
    time_hr   = sum(time_hr, na.rm = T),
    length_km = sum(length_km, na.rm = T)) |> 
  collect()

sum(d_eff$time_hr, na.rm = T)    # 302,516 hours; 12,604 days; 34.5 years
sum(d_eff$length_km, na.rm = T)  # 3,672,794 km
```



## Cleanup

```{r}
#| label: cleanup

# Close database connection
dbDisconnect(con)
```

## TODO

-   [ ] Add **indexes**.
-   [ ] Add **relationships** between primary keys.
-   [ ] Add **points** between `sites`
-   [ ] Add **segments** between `sites`.
-   [ ] Check all `type_new` are valid for db connection.
-   [ ] Check that all `flds_in` are in `flds_rn`.
-   [ ] Insert table and field SQL `COMMENT`s into database, using
    `comment` field + `workflow_md` (markdown with name and link) +
    source with tbl.fld
-   [ ] Review documentation for more comment descriptions at [Data \>
    Data Formats \|
    CalCOFI.org](https://calcofi.com/index.php?option=com_content&view=category&id=73&Itemid=993)
