---
title: "Ingest NOAA CalCOFI Database"
execute:
  echo: true
  warning: false
editor_options: 
  chunk_output_type: console
format:
  html:
    code-fold: true  
editor: 
  markdown: 
    wrap: 72
---

## Overview {.unnumbered}

**Goal**: Generate the database from source files with workflow scripts
to make updating easier and provenance fully transparent. This allows us
to:

-   Rename tables and column names, control data types and use Unicode
    encoding for a consistent database ingestion strategy, per Database
    naming conventions in [Database – CalCOFI.io
    Docs](https://calcofi.io/docs/db.html).

-   Differentiate between raw and derived or updated tables and columns.
    For instance, the taxonomy for any given species can change over
    time, such as lumping or splitting of a given taxa, and by taxonomic
    authority (e.g., WoRMS, ITIS or GBIF). These taxonomic identifiers
    and the full taxonomic hierarchy should get regularly updated
    regardless of source observational data, and can either be updated
    in the table directly or joined one-to-one with a seperate table in
    a materialized view (so as not to slow down queries with a regular
    view).

This workflow processes NOAA CalCOFI database CSV files and updates the
PostgreSQL database. The workflow:

1.  Reads CSV files from source directory
2.  Compares with lookup tables for field descriptions
3.  Initializes or updates database tables
4.  Generates summary statistics

```{mermaid}
%%| label: overview
%%| fig-cap: "Overview diagram of CSV ingestion process into the database."
%%| file: diagrams/ingest_noaa-calcofi-db_overview.mmd
```

```{r}
#| label: setup

librarian::shelf(
  DBI, dm, DT, glue, here, janitor, jsonlite, lubridate, purrr, readr, rlang, 
  tibble, tidyr, uuid,
  quiet = T)
options(readr.show_col_types = F)

# Source database connection
source(here("../apps_dev/libs/db.R"))

# define paths ---

# dataset with external data folder
dir_data <- "/Users/bbest/My Drive/projects/calcofi/data"
dataset  <- "noaa-calcofi-db"
dir_csv  <- glue("{dir_data}/{dataset}")
# input tables (version controlled)
tbls_in_csv <- here(glue("data/ingest/{dataset}/tbls_in.csv"))
flds_in_csv <- here(glue("data/ingest/{dataset}/flds_in.csv"))
# rename tables (version controlled)
tbls_rn_csv <- here(glue("data/ingest/{dataset}/tbls_rename.csv"))
flds_rn_csv <- here(glue("data/ingest/{dataset}/flds_rename.csv"))
```

## Read CSV Files

```{r}
#| label: read_csvs

# read data, extract field headers
d <- tibble(
  csv = list.files(
    dir_csv, pattern = "\\.csv$", full.names = TRUE)) |> 
  mutate(
    tbl  = tools::file_path_sans_ext(basename(csv)),
    data = map(csv, read_csv),
    nrow = map_int(data, nrow),
    ncol = map_int(data, ncol),
    flds = map2(tbl, data, \(tbl, data){
      tibble(
        fld  = names(data),
        type = map_chr(fld, \(fld) class(data[[fld]])[1] )) })) |> 
  relocate(tbl)

if (!dir.exists(dirname(tbls_in_csv)))
  dir.create(dirname(tbls_in_csv), recursive = T)

d_tbls_in <- d |> 
  select(tbl, nrow, ncol)
write_csv(d_tbls_in, tbls_in_csv)

d_flds_in <- d |> 
  select(tbl, flds) |> 
  unnest(flds) 
write_csv(d_flds_in, flds_in_csv)

datatable(d_tbls_in, caption = "Tables to ingest.")
```

```{r}
#| label: dt_flds_in

datatable(d_flds_in, caption = "Fields to ingest.")
```

## Show table and field renames

```{r}
#| label: create_or_read_flds_rn_csv

if (!file.exists(flds_rn_csv)){
  d_tbls_in |> 
    select(
      tbl_old = tbl, 
      tbl_new = tbl) |> 
    write_csv(tbls_rn_csv)

  d_flds_in |> 
    group_by(tbl) |> 
    mutate(
      fld_new  = make_clean_names(fld),
      order    = 1:n(),
      comment  = "",
      notes    = "",
      mutation = "",
      type_new = map2_chr(tbl, fld, \(x, y){
        
        v <- d |> 
          filter(
            tbl == x) |> 
          pull(data) |> 
          pluck(1) |> 
          pull(y)
        
        cl <- class(v)[1]
        
        if (cl == "character"){
          if (all(!is.na(as.UUID(v))))
            return("uuid")
        } else {
          return("varchar")
        }
        
        # integer
        if (cl == "numeric" && all(v%%1==0)){
          # https://www.postgresql.org/docs/current/datatype-numeric.html
          if (
            min(v) >= -32768 & 
            max(v) <= 32767)
            return("smallint")
          if (
            min(v) >= -2147483648 & 
            max(v) <= 2147483647)
            return("integer")
          if (
            min(v) >= -9223372036854775808 & 
            max(v) <= 9223372036854775807)
            return("bigint")
        }
        
        if (cl == "POSIXct")
          return("timestamp")
        if (cl == "Date")
          return("date")
        
        return(cl)
        })) |> 
    select(
      tbl_old   = tbl,   tbl_new = tbl,
      fld_old   = fld,   fld_new,
      order_old = order, order_new = order,
      type_old  = type,  type_new,
      comment, notes, mutation) |> 
    write_csv(flds_rn_csv)
  
  stop(glue(
    "Please update the table and field rename tables before proceeding:
      {tbls_rn_csv}
      {flds_rn_csv}"))
}

d_tbls_rn <- read_csv(tbls_rn_csv)
d_flds_rn <- read_csv(flds_rn_csv)

d_tbls_rn |>
  mutate(
    is_equal = tbl_old == tbl_new) |> 
  datatable(
    caption = "Tables to rename.",
    options = list(
      columnDefs = list(list(
        targets = "is_equal", visible = F)))) |> 
  formatStyle(
    "tbl_new",
    backgroundColor = styleEqual(
      c(T,F), 
      c("lightgray","lightgreen")),
    valueColumns    = "is_equal")
```

```{r}
#| label: dt_flds_rn

d_flds_rn |>
  mutate(
    tbl_is_equal   = tbl_old   == tbl_new,
    fld_is_equal   = fld_old   == fld_new,
    type_is_equal  = type_old  == type_new,
    order_is_equal = order_old == order_new,
    tbl = ifelse(
      tbl_is_equal,
      tbl_old,
      glue("{tbl_old} → {tbl_new}")),
    fld = ifelse(
      fld_is_equal,
      fld_old,
      glue("{fld_old} → {fld_new}")),
    type = ifelse(
      type_is_equal,
      type_old,
      glue("{type_old} → {type_new}")),
    order = ifelse(
      order_is_equal,
      order_old,
      glue("{order_old} → {order_new}"))) |> 
  select(
    -tbl_old,   -tbl_new,
    -fld_old,   -fld_new,
    -type_old,  -type_new,
    -order_old, -order_new) |> 
  relocate(tbl, fld, type, order) |> 
  datatable(
    caption = "Fields to rename.",
    rownames = F,
    options = list(
      colReorder = T,
      rowGroup = list(dataSrc = 0),
      pageLength = 50,
      columnDefs = list(list(
        targets = c(
          "tbl",
          "tbl_is_equal", "fld_is_equal", 
          "type_is_equal", "order_is_equal"), 
        visible = F))),
    extensions = c("ColReorder", "RowGroup", "Responsive")) |> 
  formatStyle(
    "fld",
    backgroundColor = styleEqual(
      c(T, F), 
      c("lightgray","lightgreen")),
    valueColumns    = "fld_is_equal") |> 
  formatStyle(
    "type",
    backgroundColor = styleEqual(
      c(T, F), 
      c("lightgray","lightgreen")),
    valueColumns    = "type_is_equal") |> 
  formatStyle(
    "order",
    backgroundColor = styleEqual(
      c(T, F), 
      c("lightgray","lightgreen")),
    valueColumns    = "order_is_equal")
```

## Apply remappings to data

```{r}
#| label: make_data_new

d0 <- d
mutate_table <- function(tbl, data) {
  # message(glue("Processing table: {tbl}"))
  
  d_f <- d_flds_rn |>
    filter(tbl_old == tbl)
  
  # rename fields ----
  f_rn <- d_f |>
    select(fld_old, fld_new) |> 
    deframe()
  
  y <- rename_with(data, ~ f_rn[.x])
  
  # mutate fields ----
  d_m <- d_f |> 
    select(fld_new, mutation) |> 
    filter(!is.na(mutation))
  
  for (i in seq_len(nrow(d_m))) {
    fld <- d_m$fld_new[i]
    mx  <- d_m$mutation[i]
    
    # message(glue("mutating {tbl}.{fld}: `{mx}`"))
    fld_sym <- rlang::sym(fld)
    mx_expr <- rlang::parse_expr(mx)
    
    y <- y |>
      mutate(!!fld_sym := eval(mx_expr, envir = y))
  }
  
  # order fields ----
  flds_ordered <- d_f |>
    arrange(order_new) |> 
    pull(fld_new)
  y <- y |>
    relocate(all_of(flds_ordered))

  return(y)
}

d <- d |>
  left_join(
    d_tbls_rn,
    by = c("tbl" = "tbl_old")) |> 
  mutate(
    data_new = map2(tbl, data, mutate_table))
```

## Load Tables into Database

```{r}
#| label: load_tbls_to_db

schema    <- "dev"
overwrite <- T

tbl_to_db <- function(tbl){ # tbl = "egg" # tbl = "ship"
  message(glue("{schema}.{tbl}  ~ {Sys.time()}" ))

  # Check if table exists
  tbl_exists <- dbExistsTable(
    con, Id(schema = schema, table = tbl))
  
  d_tbl <- d |> 
    filter(tbl_new == !!tbl) |> 
    pull(data_new) |> 
    pluck(1)
  
  v_fld_types <- d_flds_rn |> 
    filter(tbl_new == !!tbl) |> 
    arrange(order_new) |> 
    select(fld_new, type_new) |> 
    deframe()
  
  if (!tbl_exists | overwrite) {
    
    message("  loading table")
    dbWriteTable(
      con, 
      Id(schema = schema, table = tbl), 
      d_tbl, 
      field.types = v_fld_types, 
      append      = F,
      overwrite   = T)
    
  } else {
    
    message("  exists, skipping")
    
    # # Get existing data
    # existing_data <- dbGetQuery(
    #   con, sprintf("SELECT * FROM %s LIMIT 1", table_name))
    # 
    # # Compare schemas
    # new_cols <- setdiff(names(data), names(existing_data))
    # if (length(new_cols) > 0) {
    #   cat(sprintf("New columns found in %s: %s\n", table_name, paste(new_cols, collapse = ", ")))
    # }
    # 
    # # Update data - this is a simple example, you might want more sophisticated merge logic
    # dbWriteTable(con, table_name, data, overwrite = TRUE)
  }
  
  # Return summary stats
  return(tibble(
    tbl = tbl,
    nrow = nrow(d_tbl),
    ncol = ncol(d_tbl),
    dtime_ingested = Sys.time()))
}

# Process each table
tbl_stats <- map(d$tbl_new, tbl_to_db) |> 
  bind_rows()

# Display summary statistics
tbl_stats |> 
  datatable(rownames = FALSE, filter = "top")
```

## Create Indexes, Relationships

```{r}
#| label: dm_tbls

# source(here("../apps_dev/libs/db.R"))

tbls_dev <- dbListTables(con_dev) |> sort()

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")
```

### Show existing unique keys across tables

```{r}
#| label: show_keys

dm_get_all_uks(dm_dev)
```

### Add candidate keys, automatically

```{r}
#| label: mk_keys_auto

# Function to get primary key constraints for a table
tbl_pkeys <- function(con, tbl) {
  dbGetQuery(con, glue("
    SELECT a.attname as column_name
    FROM pg_index i
    JOIN pg_attribute a ON a.attrelid = i.indrelid
    AND a.attnum = ANY(i.indkey)
    WHERE i.indrelid = '{tbl}'::regclass
    AND i.indisprimary;
  "))$column_name
}

d_pk <- tibble(
  tbl = tbls_dev) |> 
  mutate(
    d        = map(tbl, \(x) eval(bquote(dm_enum_pk_candidates(dm_dev, .(x)))) ),
    n        = map_int(d, \(x) x |> filter(candidate == T) |> nrow()),
    flds     = map_chr(d, \(x){
      x |> filter(candidate == T) |> pull(columns) |> 
        unlist() |> paste(collapse = "; ") }),
    fld      = map_chr(d, \(x){
      y <- x |> filter(candidate == T) |> pull(columns) |> 
        unlist()
      if (length(y) == 0)
        return(NA)
      if (length(y) > 1)
        return(y[1])
      y }),
    add_pkey = map2_lgl(tbl, fld, \(tbl, fld){
      if (is.na(fld))
        return(F)
      # tbl <- "cruise"; fld = "cruise_uuid"
      pkey_exists <- tbl_pkeys(con_dev, "cruise") |> length() > 0
      if (pkey_exists)
        return(F)
      dbExecute(con_dev, glue("ALTER TABLE {tbl} ADD PRIMARY KEY ({fld})"))
      return(T) }) ) |> 
  select(-d)
    
d_pk |> 
  datatable()
```

### Add primary keys, manually

Noticing egg and larva missing pkey.

```{r}
#| label: mk_keys_manual

dbExecute(con_dev, "ALTER TABLE egg ADD PRIMARY KEY (net_uuid, sp_id)")
dbExecute(con_dev, "ALTER TABLE larva ADD PRIMARY KEY (net_uuid, sp_id)")

tibble(
  tbl = tbls_dev) |> 
  mutate(
    pkeys = map_chr(tbl, \(x) tbl_pkeys(x, con = con_dev) |> paste(collapse = ", "))) |> 
  datatable()
```

### Add other indexes

That are not already the first field of a primary key.

```{r}
#| label: mk_idx

d_idx <- tibble(
  tbl = tbls_dev) |>
  mutate(
    fld   = map(tbl, ~dbListFields(.x, conn = con_dev)),
    pkeys = map(tbl, ~tbl_pkeys(.x, con = con_dev))) |> 
  unnest(fld) |>
  mutate(
    is_pkey  = map2_lgl(fld, pkeys, `%in%`),
    is_pkey1 = map2_lgl(fld, pkeys, \(fld, pkeys) fld == pkeys[1]),
    is_id    = map_lgl(fld, ~str_detect(.x, "_(id|uuid|tsn)$")),
    idx_todo = !is_pkey1 & is_id,
    mk_idx   = pmap_lgl(list(tbl, fld, idx_todo), \(tbl, fld, idx_todo){
      if (idx_todo){
        q <- glue("CREATE INDEX IF NOT EXISTS idx_{tbl}_{fld} ON {tbl} ({fld})")
        # message(q)
        dbExecute(con_dev, q)
        return(T)
      }
      return(F)
    }))

d_idx |> 
  select(-pkeys) |> 
  datatable()
```

### Add foreign keys

egg.net_uuid -> net.net_uuid .tow_uuid -> tow.tow_uuid .site_uuid -> site.site_uuid .cruise_uuid -> cruise.cruise_uuid .ship_key -> ship.ship_key

```{r}
#| label: dm_fk

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")

dm_dev_fk <- dm_dev |> 
  dm_add_fk(egg, net_uuid, net) |> 
  dm_add_fk(larva, net_uuid, net) |> 
  dm_add_fk(net, tow_uuid, tow) |> 
  dm_add_fk(tow, site_uuid, site) |> 
  dm_add_fk(site, cruise_uuid, cruise) |> 
  dm_add_fk(cruise, ship_key, ship) |> 
  dm_add_fk(tow, tow_type_key, tow_type) |> 
  dm_add_fk(egg, sp_id, species) |> 
  dm_add_fk(larva, sp_id, species) 
dm_draw(dm_dev_fk, view_type = "all")
```

```{r}
#| label: sql_fk

add_fkey <- function(tbl_m, fld_m, tbl_1, fld_1 = fld_m, schema = "dev"){
  q <- glue(
    "ALTER TABLE {schema}.{tbl_m} ADD FOREIGN KEY ({fld_m}) 
    REFERENCES {schema}.{tbl_1} ({fld_1})")
  dbExecute(con_dev, q)
}

add_fkey("egg", "net_uuid", "net")
add_fkey("larva", "net_uuid", "net") 
add_fkey("net", "tow_uuid", "tow") 
add_fkey("tow", "site_uuid", "site")
add_fkey("site", "cruise_uuid", "cruise")
add_fkey("cruise", "ship_key", "ship")
add_fkey("tow", "tow_type_key", "tow_type")
add_fkey("egg", "sp_id", "species")
add_fkey("larva", "sp_id", "species")

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")
```

```
Error: Failed to fetch row : ERROR:  insert or update on table "larva" violates foreign key constraint "larva_sp_id_fkey"
DETAIL:  Key (sp_id)=(3023) is not present in table "species".
```

```{r}
#| label: sp_missing

sp_missing_csv <- here(
    "data/ingest/noaa-calcofi-db/larvae-species_not-in-species.csv")

d_sp_missing <- tbl(con_dev, "larva") |> 
  anti_join(
    tbl(con_dev, "species"),
    by = "sp_id") |> 
  group_by(sp_id) |> 
  summarize(
    n_rows = n()) |> 
  left_join(
    dm_dev |>
      dm_select_tbl(larva, net, tow, site) |> 
      dm_flatten_to_tbl(.start = larva, .recursive = TRUE) |> 
      select(sp_id, tally, time_start, line, station, cruise_uuid) |> 
      group_by(sp_id) |> 
      summarize(
        sum_tally  = sum(tally),
        date_beg   = min(as.Date(time_start)),
        date_end   = max(as.Date(time_start)),
        # ship_names = str_flatten(distinct(name), collapse = "; ")), 
        n_cruises = n_distinct(cruise_uuid)), 
    by = "sp_id") |> 
  collect()

write_csv(d_sp_missing, sp_missing_csv)

datatable(d_sp_missing)
```


```{r}
#| label: delete_larva_sp_missing

dbExecute(con_dev, glue(
  "DELETE FROM larva WHERE sp_id IN ({paste(d_sp_missing$sp_id, collapse = ',')})"))
add_fkey("larva", "sp_id", "species")

dm_dev <- dm_from_con(
  con_dev, 
  table_names = tbls_dev,
  learn_keys  = T)
dm_draw(dm_dev, view_type = "all")
```

## Add Spatial

### Add `site.geom`

```{r}
#| label: mk_site_pts

# source(here("../apps_dev/libs/db.R"))
# dbGetQuery(con_dev, "SELECT postgis_full_version()")

dbExecute(con_dev, "ALTER TABLE dev.site ADD COLUMN geom geometry(Point, 4326)")
dbExecute(con_dev, "UPDATE dev.site SET geom = ST_SetSRID(ST_MakePoint(longitude, latitude), 4326)") # 61,220 rows
```

### Fix calcofi4r `grid`

Problems with [calcofi4r::`cc_grid`](https://calcofi.io/calcofi4r/articles/calcofi4r.html):

- uses old station (line, position) vs newer site (line, station)
- `sta_lin`, `sta_pos`: integer, so drops necessary decimal that is found in `sta_key`
- `sta_lin == 90, sta_pos == 120` repeats for:
  * `sta_pattern` == 'historical' (`sta_dpos` == 20); and 
  * `sta_pattern` == 'standard'   (`sta_dpos` == 10)

```{r}
#| label: mk_grid_v2

librarian::shelf(
  calcofi4r, dplyr, mapview, sf, tidyr, units)

cc_grid_v2 <- calcofi4r::cc_grid |> 
  select(sta_key, shore = sta_shore, pattern = sta_pattern, spacing = sta_dpos) |>
  separate_wider_delim(
    sta_key, ",", names = c("line", "station"), cols_remove = F) |> 
  mutate(
    line    = as.double(line),
    station = as.double(station),
    grid_key = ifelse(
      pattern == "historical", 
      glue("st{station}-ln{line}_hist"),
      glue("st{station}-ln{line}")),
    zone = glue("{shore}-{pattern}")) |> 
  relocate(grid_key, station) |> 
  st_as_sf() |> 
  mutate(
    area_km2 = st_area(geom) |>
      set_units(km^2) |>
      as.numeric())

cc_grid_ctrs_v2 <- calcofi4r::cc_grid_ctrs |> 
  select(sta_key, pattern = sta_pattern) |> 
  left_join(
    cc_grid_v2 |> 
      st_drop_geometry(),
    by = c("sta_key", "pattern")) |> 
  select(-sta_key) |> 
  relocate(grid_key)
  
cc_grid_v2 <- cc_grid_v2 |> 
  select(-sta_key)

cc_grid_v2 |> 
  st_drop_geometry() |> 
  datatable()

mapview(cc_grid_v2, zcol="zone") +
  mapview(cc_grid_ctrs_v2, cex = 1)
```

```{r}
#| label: grid_to_db

grid <- cc_grid_v2 |> 
  as.data.frame() |> 
  left_join(
    cc_grid_ctrs_v2 |> 
      as.data.frame() |> 
      select(grid_key, geom_ctr = geom),
    by = "grid_key") |> 
  st_as_sf(sf_column_name = "geom")

grid |> 
  st_write(con_dev, "grid")
dbExecute(con_dev, glue(
  "CREATE INDEX IF NOT EXISTS idx_grid_geom ON grid USING gist(geom);"))
dbExecute(con_dev, "ALTER TABLE grid ADD PRIMARY KEY (grid_key)")
```

### Update `site.grid_key`

```{r}
#| label: update_site_from_grid

dbExecute(con_dev, "ALTER TABLE site ADD COLUMN grid_key text")

dbExecute(con_dev, "
  UPDATE site
  SET
    grid_key = g.grid_key
  FROM (
    SELECT s.site_uuid, g.grid_key
    FROM site s
    INNER JOIN grid g ON ST_Intersects(s.geom, g.geom)) g
  WHERE site.site_uuid = g.site_uuid") # 59,136

# tbl(con_dev, "site") |>
#   mutate(is_in_grd = !is.na(grid_key)) |>
#   pull(is_in_grd) |>
#   table()
#  FALSE    TRUE 
#  2,084  59,136

dbExecute(con_dev, glue(
  "CREATE INDEX IF NOT EXISTS idx_site_geom_key ON grid (grid_key);"))
add_fkey("site", "grid_key", "grid")

dm_from_con(
  con_dev, 
  # table_names = dbListTables(con_dev_only),
  table_names = c("site", "grid"),
  learn_keys  = T) |> 
  dm_set_colors(green = grid) |>
  dm_draw(view_type = "all")
```

### Add `site_seg`: segments from site

```{r}
#| label: mk_site_seg

site_seg <- tbl(con_dev, "site") |> 
  select(cruise_uuid, orderocc, site_uuid, lon = longitude, lat = latitude) |> 
  left_join(
    tbl(con_dev, "tow") |> 
      select(site_uuid, time_start),
    by = "site_uuid") |>
  arrange(cruise_uuid, orderocc) |> 
  group_by(
    cruise_uuid, orderocc, site_uuid, lon, lat) |> 
  summarize(
    time_beg = min(time_start, na.rm = T),
    time_end = max(time_start, na.rm = T),
    .groups = "drop") |> 
  collect() |> 
# table(is.na(site_seg$time_beg))
#   FALSE   TRUE 
#  57,935  3,285
site_seg <- site_seg |>   
  arrange(cruise_uuid, orderocc, time_beg) |> 
  group_by(cruise_uuid) |> 
  mutate(
    site_uuid_beg = lag(site_uuid),
    lon_beg       = lag(lon),
    lat_beg       = lag(lat),
    time_beg      = lag(time_beg)) |> 
  ungroup() |> 
  filter(!is.na(lon_beg), !is.na(lat_beg)) |> 
  mutate(
    m    = pmap(
      list(lon_beg, lat_beg, lon, lat), 
      \(x1, y1, x2, y2){
        matrix(c(x1, y1, x2, y2), nrow = 2, byrow = T) }),
    geom = map(m, st_linestring)) |> 
  select(
    cruise_uuid,
    site_uuid_beg, 
    site_uuid_end = site_uuid, 
    lon_beg, 
    lat_beg,
    lon_end = lon, 
    lat_end = lat,
    time_beg, 
    time_end, 
    geom) |> 
  st_as_sf(
    sf_column_name = "geom", 
    crs = 4326) |> 
  mutate(
    time_hr   = as.numeric(difftime(time_end, time_beg, units = "hours")),
    length_km = st_length(geom) |>
      set_units(km) |>
      as.numeric(),
    km_per_hr = length_km / time_hr)

# TODO: check that time_end of previous segment is time_beg of next

# TODO: investigate too slow, too fast
# site_seg |> 
#   mutate(
#     km_per_hr_flag = ifelse(
#       km_per_hr < 0.01,
#       "lt0.01",
#       ifelse(
#         km_per_hr > 30,
#         "gt30",
#         NA))) |> 
#   pull(km_per_hr_flag) |> 
#   table()
#   gt30 lt0.01 
#    928      9

fld_types <- tibble(
  lst = lapply(site_seg, class)) |> 
  mutate(
    fld    = names(lst),
    type_r = map_chr(lst, pluck, 1),
    type   = map2_chr(fld, type_r, \(fld, type_r){
      case_when(
        str_detect(fld, "uuid") ~ "uuid",
        fld    == "geom"        ~ "geometry",
        type_r == "POSIXct"     ~ "timestamp",
        type_r == "character"   ~ "varchar",
        .default = "numeric") })) |> 
  select(fld, type) |> 
  deframe()
st_write(site_seg, con_dev, "site_seg", field.types = fld_types)
dbExecute(con_dev, "CREATE INDEX IF NOT EXISTS idx_site_seg_geom ON grid USING gist(geom)")
dbExecute(con_dev, "ALTER TABLE site_seg ADD PRIMARY KEY (site_uuid_beg)")
dbExecute(con_dev, 'CREATE EXTENSION IF NOT EXISTS "uuid-ossp"')
add_fkey("site_seg", "site_uuid_beg", "site", "site_uuid")
add_fkey("site_seg", "site_uuid_end", "site", "site_uuid")
add_fkey("site_seg", "cruise_uuid", "cruise")

dm_from_con(
  con_dev, 
  # table_names = dbListTables(con_dev_only),
  table_names = c("site_seg", "site", "cruise"),
  learn_keys  = T) |> 
  dm_set_colors(green = site_seg) |>
  dm_draw(view_type = "all")

site_seg <- st_read(con_dev, "site_seg") |> 
  mutate(
    year = year(time_beg))

mapView(site_seg, zcol = "year")
```

## bottle-database

TODO: 

- [ ] use additional time-location information for updating `site_seg`
- [ ] split into seperate workflow
  

```{r}
#| label: ingest_bottle-database

dataset      <- "bottle-database"
dir_dataset  <- glue("{dir_data}/oceanographic-data/{dataset}")
dir_ingest   <- here(glue("data/ingest/{dataset}"))
bottle_csv   <- glue("{dir_dataset}/CalCOFI_Database_194903-202105_csv_16October2023/194903-202105_Bottle.csv")
cast_csv     <- glue("{dir_dataset}/CalCOFI_Database_194903-202105_csv_16October2023/194903-202105_Cast.csv")
# Open "Bottle Field Descriptions.csv" in Excel and Save As File Format: CSV UTF-8 (Comma delimited) (.csv)
m_bottle_csv <- glue("{dir_dataset}/Bottle Field Descriptions - UTF-8.csv")
m_cast_csv   <- glue("{dir_dataset}/Cast Field Descriptions.csv")
flds_rn_csv  <- glue("{dir_ingest}/flds_rename.csv")
  
if (!file.exists(flds_rn_csv)){
  
  # metadata
  m <- bind_rows(
    read_csv(m_cast_csv) |> 
      mutate(tbl = "cast") |> 
      relocate(tbl) |> 
      clean_names(),
    read_csv(m_bottle_csv) |>
      mutate(tbl = "bottle") |> 
      relocate(tbl) |> 
      clean_names()) |> 
    rename(
      fld_csv     = field_name) |> 
    mutate(
      fld_clean   = make_clean_names(fld_csv),
      fld_db      = fld_clean,
      ingest_note = "",
      comment     = pmap_chr(
        list(description, units, fld_csv), 
        \(description, units, fld_csv){
          list(
            description = description, 
            units       = units, 
            source      = glue("{basename(bottle_csv)}.{fld_csv}")) |> 
            toJSON(auto_unbox=T) })) |> 
    relocate(fld_clean, fld_db, note, .after = "fld_csv")
  
  write_excel_csv(m, flds_rn_csv) # UTF-8 encoding
  
  message(glue(
    "Please modify columns for data ingestion:
       {flds_rn_csv}
     - fld_db: (optionally making blank to exclude); 
     - note: notes for ingestion; and 
     - comment: for metadata description in JSON format"))
}
m <- read_csv(flds_rn_csv)
```

```{r}
#| label: fix ship.ship_nodc IS NA

dbExecute(con_dev, "UPDATE ship SET ship_nodc = '32BH' WHERE ship_key = 'BH' AND ship_nodc IS NULL")
stopifnot(tbl(con_dev, "ship") |>  filter(is.na(ship_nodc)) |> collect() |> nrow() == 0)
```

```{r}
#| label: cast.ship_nodc missing in ship

d_cast <- read_csv(cast_csv) |> 
  clean_names()

d_cast_ship_miss <- d_cast |>
  select(ship_name, ship_nodc = ship_code) |> 
  group_by(ship_name, ship_nodc) |> 
  summarize(
    n_casts = n(),
    .groups = "drop") |> 
  anti_join(
    tbl(con_dev, "ship") |> 
      collect(),
    by = "ship_nodc")
#   ship_name             ship_nodc n_casts
#   <chr>                 <chr>       <int>
# 1 ARGO                  31AR            4
# 2 DAVID PHILLIP DOLPHIN 33DP           34
# 3 RV BELL M SHIMADA     325S          670
# 4 RV BELL M. SHIMADA    325S           67
# 5 RV DAVID STARR JORDAN 03JD            1
# 6 RV OCEAN STARR        31OS          213
# 7 RV REUBEN LASKER      33RL          464
# 8 RV SALLY RIDE         33SR          514
# 9 WESTWIND              32WC           11
d_cast_ship_miss
```
  
  
```{r}
#| label: get_ship_nodc_refs

librarian::shelf(dplyr, purrr, readr, rvest)

ships_cc_url <- "https://www.calcofi.info/index.php/field-work/calcofi-ships/unols-ship-codes"
d_ships_cc <- tibble(
  url = read_html(ships_cc_url) |> 
    html_nodes("table a") |> 
    html_attr("href") |> 
    keep(~ grepl("/unols-ship-codes/\\d+-", .x)) |>   # Filter valid ship code links
    map_chr(~ paste0("https://www.calcofi.info", .x))) |> 
  mutate(
    data = map(
      url, \(url){
        read_html(url) |> 
          html_node("table") |>
          html_table(header = T) })) |> 
  unnest(data) |> 
  clean_names() |> 
  select(ship_nodc = ship_code, ship_name = ship, remarks, source_url = url) |> 
  mutate(
    remarks = na_if(remarks, ""))
write_csv(d_ships_cc, here("data/ships_calcofi.csv"))

ships_nodc_url <- "https://www.nodc.noaa.gov/OC5/WOD/CODES/s_3_platform.html"
d_ships_nodc <- read_html(ships_nodc_url) |> 
  html_node("table")  |> 
  html_table(header = T) |> 
  clean_names() |> 
  select(ship_nodc = nodc_code, p = platform_name) |> 
  mutate(
    ship_name = str_replace(p, "(.*) \\(.*\\)", "\\1"),
    remarks   = map_chr(p, \(p){
      if (!str_detect(p, "\\(")) return(NA)
      str_replace(p, "(.*) \\((.*)\\)", "\\2") }),
    source_url = ships_nodc_url) |> 
  select(ship_nodc, ship_name, remarks, source_url)
write_csv(d_ships_nodc, here("data/ships_nodc.csv"))
```


```{r}
d_ships <- c(
  cc   = "data/ships_calcofi.csv",
  nodc = "data/ships_nodc.csv") |> 
  enframe(name = "src", value = "csv") |> 
  mutate(
    data = map(csv, read_csv)) |> 
  unnest(data) |> 
  select(-csv, -source_url) # 15,937 × 4
#   src   ship_nodc ship_name       remarks                         
#   <chr> <chr>     <chr>           <chr>                           
# 1 cc    90UW      11TH PJATILETKA OCL REQUEST                     
# 2 cc    90TU      1500 LET KIYEVU LYFB                            
# 3 cc    90FE      20-21           OCL                             
# 4 cc    90JO      203             ICES REQUEST  07Oct38 to 12Nov38
# 5 cc    73DE      30 DECEMBER     ICES REQUEST (years 1960 - 1961)
# 6 cc    76AJ      3-101           NA                              

db_ship <- tbl(con_dev, "ship") |> 
  collect() |> 
  rename(db_key = ship_key, db_name = ship_name, db_code = ship_nodc)

librarian::shelf(fuzzyjoin)

ck <- function(val, i, src, fld){
  if (fld == "ship_nodc")
    y <- d_ships |> filter(src == !!src, ship_nodc == !!val) |> pull(ship_name)
  if (fld == "ship_name")
    y <- d_ships |> filter(src == !!src, ship_name == !!val) |> pull(ship_nodc)
  if (length(y) == 0){
    y <- NA
  } else {
    y <- paste(y, collapse = "; ")
  }
  y
}

d_ship_matches <- d_cast_ship_miss |> 
  rename(csv_name = ship_name, csv_code = ship_nodc) |> 
  mutate(
    code_in_cc_name   = imap_chr(csv_code, ck, src="cc",   fld="ship_nodc"),
    code_in_nodc_name = imap_chr(csv_code, ck, src="nodc", fld="ship_nodc"),
    name_in_cc_code   = imap_chr(csv_name, ck, src="cc",   fld="ship_name"),
    name_in_nodc_name = imap_chr(csv_name, ck, src="nodc", fld="ship_name"),
    name_similar_db   = map_chr(csv_name, \(csv_name){
      wds_csv = str_split(csv_name, " ")[[1]]
      
      y <- db_ship |> 
        mutate(
          wds_db       = map_vec(db_name, str_split, " "),
          n_wds_match  = map_int(wds_db, \(wds_db){
            sum(wds_csv %in% wds_db) })) |> 
        filter(n_wds_match > 1) |> 
        arrange(desc(n_wds_match)) |> 
        head(1) |> 
        pull(db_name)
      if (length(y) == 0)
        return(NA)
      y })) |> 
  left_join(
    db_ship |> 
      select(name_similar_db = db_name, db_code),
    by = "name_similar_db")

d_ship_matches <- read_csv(glue("{dir_ingest}/ship_renames.csv"))
View(d_ship_matches)

%in% (  |> pull()),
    csv_code_in_nodc = csv_code %in% (d_ships |> filter(src == "nodc") |> pull(ship_nodc)),
    csv_name_in_cc   = csv_name %in% (d_ships |> filter(src == "cc")   |> pull(ship_name)),
    csv_name_in_nodc = csv_name %in% (d_ships |> filter(src == "nodc") |> pull(ship_name)))

rx_cruise <- "^([0-9]{4})-([0-9]{2})-([0-9]{2})-C-([0-9A-Z]{4})$"
d_cast |>
  mutate(
    date_ym   = str_replace(cruise_id, rx_cruise, "\\1-\\2-01") |> as.Date(),
    ship_nodc = str_replace(cruise_id, rx_cruise, "\\4")) |> 
  distinct(nodc) |> 
  anti_join(
    tbl(con_dev, "ship") |> 
      collect(),
    by = "nodc")

d_cast |>
  filter()
  select(ship_name, ship_nodc = ship_code) |> 
  distinct(ship_name, ship_nodc) |> 
  filter(ship_nodc == "31JD")
  anti_join(
    tbl(con_dev, "ship") |> 
      arrange(ship_name) |> 
      collect() |> 
      View(),
    by = "ship_nodc")

# confirm:
# - [Field Work > CalCOFI Ships > UNOLS Ship Codes](https://www.calcofi.info/index.php/field-work/calcofi-ships/unols-ship-codes)
# - [World Ocean Database: code tables](https://www.nodc.noaa.gov/OC5/WOD/CODES/s_3_platform.html)
#   ship_name             ship_nodc  status
# 1 ARGO                  31AR       new
# 2 DAVID PHILLIP DOLPHIN 33DP       new  
# 3 WESTWIND              32WC       new: 32WC -> 31WE
# 4 RV DAVID STARR JORDAN 03JD       old: 03JD -> 31JD
# 5 RV BELL M SHIMADA     325S       ship tbl: BELL M. SHIMADA 3322
  # WODC only: 3322	BELL M. SHIMADA (F/R/V;call sign WTED;built 10.2009;IMO9349069)
  # WODC,CalCOFI: 325S	SHIMADA
# 6 RV BELL M. SHIMADA    325S     
# 7 RV OCEAN STARR        31OS 	     ship: OCEAN STARR 32I1 (WODC)
# 8 RV REUBEN LASKER      33RL       ship: REUBEN LASKER 33UD (WODC)
# 9 RV SALLY RIDE         33SR       ship: SALLY RIDE 33P4 (WODC)
# NEW:
# - ARGO [31AR]
# - DAVID PHILLIP DOLPHIN [33DP]
# TYPOS:
# - RV DAVID STARR JORDAN [03JD] -> DAVID STARR JORDAN [31JD]
d_ships <- read_csv(here("data/calcofi-ships.csv"))  
d_ships |> 
  filter(ship_code == "31AR")


tbl(con_dev, "ship") |> 
  pull(nodc) |> nchar() |> table()
tbl(con_dev, "cruise") |> 
  pull(ship_key) |> unique() |> sort() |> table()


tbl(con_dev, "cruise") |> 
  left_join(
    tbl(con_dev, "ship") |> pull(nodc),
    by = "ship_key") |> 
  
  filter(
    nodc == "32BH") | is.na(nodc))



d_bottle <- read_csv(bottle_csv) |> 
  clean_names()

View(d_bottle)



```


## Report

```{r}
#| label: show_latest

dm_from_con(
  con_dev, 
  table_names = dbListTables(con_dev_only),
  # table_names = c("site_seg", "site", "cruise"),
  learn_keys  = T) |> 
  dm_set_colors(lightgreen = c(site_seg, grid)) |>
  dm_draw(view_type = "all")

d_eff <- tbl(con_dev, "site_seg") |> 
  mutate(
    year = year(time_beg)) |> 
  group_by(year) |> 
  summarize(
    time_hr   = sum(time_hr, na.rm = T),
    length_km = sum(length_km, na.rm = T)) |> 
  collect()

sum(d_eff$time_hr, na.rm = T)    # 302,516 hours; 12,604 days; 34.5 years
sum(d_eff$length_km, na.rm = T)  # 3,672,794 km
```



## Cleanup

```{r}
#| label: cleanup

# Close database connection
dbDisconnect(con)
```

## TODO

-   [ ] Add **indexes**.
-   [ ] Add **relationships** between primary keys.
-   [ ] Add **points** between `sites`
-   [ ] Add **segments** between `sites`.
-   [ ] Check all `type_new` are valid for db connection.
-   [ ] Check that all `flds_in` are in `flds_rn`.
-   [ ] Insert table and field SQL `COMMENT`s into database, using
    `comment` field + `workflow_md` (markdown with name and link) +
    source with tbl.fld
-   [ ] Review documentation for more comment descriptions at [Data \>
    Data Formats \|
    CalCOFI.org](https://calcofi.com/index.php?option=com_content&view=category&id=73&Itemid=993)
