---
title: "CalCOFI Data Workflow Plan"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    embed-resources: true
---
<style>
.mermaid-js rect{
  fill: lightyellow!important;
}
</style>

<!--
PROMPT to Claude 2026-01-31:

Help me write a new a data workflow plan to manage and version raw data files, an integrated database and workflows to ingest raw data in and publish datasets out.

-   Use documents in the @workflows/_docs folder as suggestions for technologies (with technological preference where in conflict given to the more recent files as given by their date prefix filename).

-   We already have a server on the Google Cloud platform, so thinking of using Google Cloud Storage buckets and a new service account for managing that (rather than AWS S3).

-   We already have workflows in @workflows, but there are not yet dependencies built between them. Nor is the sequence anywhere stated. For instance, to build the integrated database thus far requires running first @workflows/ingest_calcofi.org_bottle-database.qmd, then @workflows/ingest_calcofi.org_bottle-database.qmd (once completed). How can these be better organized and developed as a dataset is initially explored, then ingested, then updated?

-   Consider using R package `targets` to manage dependencies between files and workflows, and `pins` for sharing and versioning files.

-   Review and suggest revisions to the existing documentation under @docs/, especially on Database, API and Portals. Could the API and Postgres database be superceded by use of parquet files and DuckDB?

-   Describe relevance, strategy and purpose of R packages @calcofi4r and @calcofi4db.

-   How can we systematically reuse documented functions in R libraries across workflows, apps, database access, etc?

-   For now, write this to @workflows/README_PLAN.md. Eventually, a more succinct version will probably be folded into @workflows/README.md, but also looking for suggestions, such as perhaps more appropriate in the R libraries @calcofi4r or @calcofi4db and/or @docs.

-   Be sure to include a systematic plan for implementation, including what should be refactored first. Prioritize producing a tidy, clean version of the integrated database with ingestion from: @workflows/ingest_calcofi.org_bottle-database.qmd, @workflows/ingest_calcofi.org_bottle-database.qmd; and output to @workflows/publish_larvae_to_obis.qmdpublish_larvae_to_obis.qmd (which does not yet include the bottle data). Then get this example to work from storing source files in a versioned cloud storage bucket, to referencing these files in the ingest scripts (perhaps with helper functions to get latest cloud version file, given state of Google Drive), producing and uploading parquet files to cloud storage, registering new versions with a DuckLake, and producing helper functions to get latest (or past) versions of the database as relational duckdb with metadata as COMMENTS and foreign keys registered.
-->

> This document outlines a comprehensive strategy for managing and versioning CalCOFI data files, building an integrated database, and publishing datasets to external portals.

## Overview for Data Managers & Scientists {#sec-overview}

This section provides a high-level view of the CalCOFI data system. **You don't need to understand all the technical details**‚Äîjust the four main components and how they connect.

### The Big Picture

```{mermaid}
%%| label: fig-overview
%%| fig-cap: "CalCOFI Data System Overview"

flowchart TB
    subgraph drive["üìÅ THE DRIVE"]
        direction LR
        gd_pub["data-public/<br/>Bottle, Larvae, CTD..."]
        gd_priv["data-private/<br/>eDNA, Zooplankton..."]
    end

    subgraph archive["üóÑÔ∏è THE ARCHIVE"]
        direction LR
        sync["_sync/<br/>(working copy)"]
        hist["archive/<br/>(snapshots)"]
    end

    subgraph workflows["‚öôÔ∏è THE WORKFLOWS"]
        direction LR
        ingest["Ingest<br/>(CSV ‚Üí DB)"]
        publish["Publish<br/>(DB ‚Üí Portals)"]
    end

    subgraph database["üóÉÔ∏è THE DATABASE"]
        direction LR
        working["Working<br/>(internal)"]
        frozen["Frozen<br/>(public releases)"]
    end

    subgraph outputs["üåê OUTPUTS"]
        direction LR
        obis["OBIS"]
        apps["Shiny Apps"]
        api["API / R pkg"]
    end

    drive -->|"daily sync"| archive
    archive -->|"read files"| workflows
    workflows -->|"transform"| database
    database -->|"publish"| outputs

    style drive fill:#e8f4e8,stroke:#2e7d32
    style archive fill:#e3f2fd,stroke:#1565c0
    style workflows fill:#fff3e0,stroke:#ef6c00
    style database fill:#f3e5f5,stroke:#7b1fa2
    style outputs fill:#fce4ec,stroke:#c2185b
```

### Four Key Components

::: {.callout-note collapse="false"}
## üìÅ The Drive

**What**: The shared Google Drive folder where you upload your data files.

**Location**: `My Drive/projects/calcofi/data-public/` (or `data-private/`)

**Who uses it**: Dataset managers (Rasmus, Ed, Nastassia, Linsey, etc.)

**How to use it**:

1. Create a folder for your data: `{provider}/{dataset}/` (e.g., `swfsc.noaa.gov/calcofi-db/`)
2. Inside, organize files into `raw/` (original data) and `derived/` (processed data)
3. Simply upload or overwrite files‚Äîversioning happens automatically
4. No special naming required (no date prefixes needed)

**Example structure**:
```
data-public/
‚îú‚îÄ‚îÄ calcofi.org/
‚îÇ   ‚îî‚îÄ‚îÄ bottle-database/
‚îÇ       ‚îú‚îÄ‚îÄ raw/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ bottle_export_2026-01-15.csv
‚îÇ       ‚îî‚îÄ‚îÄ derived/
‚îÇ           ‚îî‚îÄ‚îÄ bottle_cleaned.csv
‚îú‚îÄ‚îÄ swfsc.noaa.gov/
‚îÇ   ‚îî‚îÄ‚îÄ calcofi-db/
‚îÇ       ‚îú‚îÄ‚îÄ raw/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ larva.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ cruise.csv
‚îÇ       ‚îî‚îÄ‚îÄ derived/
```
:::

::: {.callout-note collapse="false"}
## üóÑÔ∏è The Archive

**What**: A versioned backup of all Drive files in the cloud[^datalake].

**Location**: Google Cloud Storage buckets (`gs://calcofi-files-public/` and `gs://calcofi-files-private/`)

**Who uses it**: Automated systems (you don't need to interact with it directly)

**What it does**:

- **Daily sync**: Copies all files from the Drive to the Archive
- **Snapshots**: Creates timestamped copies so we can always go back in time
- **Never loses data**: Even if you delete a file from Drive, it's preserved in the Archive

**Why it matters**: Scientists can reproduce analyses using the exact data from any past date.

[^datalake]: Technical term: This is a "versioned data lake" built on Google Cloud Storage with rclone synchronization.
:::

::: {.callout-note collapse="false"}
## ‚öôÔ∏è The Workflows

**What**: Automated scripts that process data from the Archive into the Database.

**Location**: `/workflows/` folder (Quarto notebooks and R scripts)

**Who uses it**: Data engineers and developers

**Two types of workflows**:

1. **Ingest workflows** (`ingest_*.qmd`): Read CSV files ‚Üí Clean/transform ‚Üí Load into Database
2. **Publish workflows** (`publish_*.qmd`): Read from Database ‚Üí Format for portals ‚Üí Upload to OBIS, EDI, etc.

**Key feature**: Dependencies are managed automatically‚Äîif source data changes, all downstream steps re-run.
:::

::: {.callout-note collapse="false"}
## üóÉÔ∏è The Database

**What**: The integrated CalCOFI database combining all datasets into related tables.

**Format**: DuckDB[^duckdb] with Parquet files (modern, fast, serverless)

**Two versions**:

| Version | Purpose | Access | Features |
|---------|---------|--------|----------|
| **Working Database** | Internal processing | Workflows only | Full provenance tracking, UUIDs, source file references, time travel |
| **Frozen Database** | Public releases | Apps, API, R package | Clean tables, versioned releases (v2026.02, v2026.03...), optimized for queries |

**Why two versions?** The Working Database tracks every detail for reproducibility. The Frozen Database provides clean, stable snapshots for users who just want the data.

[^duckdb]: [DuckDB](https://duckdb.org/) is a fast, in-process database that can query Parquet files directly from cloud storage without downloading them.
:::

### How Data Flows

```{mermaid}
%%| label: fig-data-flow-simple
%%| fig-cap: "Simplified Data Flow"

flowchart TB
    subgraph you["<b>üë§ YOU (Dataset Manager)</b>"]
        upload["Upload CSV to Drive"]
    end

    subgraph auto["<b>ü§ñ AUTOMATED (Daily)</b>"]
        sync["Sync to Archive"]
        snapshot["Create snapshot"]
        transform["Run workflows"]
    end

    subgraph result["<b>üìä RESULT</b>"]
        db["Updated Database"]
        portals["Published to Portals"]
    end

    upload --> sync --> snapshot --> transform --> db --> portals

    style you fill:#e8f4e8,stroke:#2e7d32
    style auto fill:#fff3e0,stroke:#ef6c00
    style result fill:#f3e5f5,stroke:#7b1fa2
```

**Your role as a dataset manager**: Upload files to the Drive. Everything else happens automatically.

---

## Executive Summary

This plan modernizes the CalCOFI data workflow architecture to:

1. Version raw data files in Google Cloud Storage (GCS)
2. Use DuckDB + Parquet as the primary integrated database format
3. Leverage `targets` for workflow dependency management
4. Use R packages (`calcofi4r`, `calcofi4db`) for reusable functions
5. Maintain reproducible pipelines from raw data to published datasets

---

## Current State Analysis

### Existing Infrastructure

| Component | Current State | Location |
|-----------|--------------|----------|
| **Workflows** | Individual Quarto notebooks | `/workflows/` |
| **Database** | PostgreSQL with dev/prod schemas | api.calcofi.io |
| **R Packages** | `calcofi4r` (user), `calcofi4db` (admin) | GitHub |
| **Data Sources** | CSV files on Google Drive | `~/My Drive/projects/calcofi/data-public` |
| **Documentation** | Quarto site | `/docs/` |

### Key Workflow Files

| Workflow | Purpose | Status |
|----------|---------|--------|
| `ingest_calcofi.org_bottle-database.qmd` | Ingest bottle/cast data | Partial |
| `ingest_swfsc.noaa.gov_calcofi-db.qmd` | Ingest NOAA ichthyoplankton data | Active |
| `publish_larvae_to_obis.qmd` | Publish larvae to OBIS | Active (needs bottle data) |

### Identified Gaps

1. **No dependency management**: Workflows run independently without declared dependencies
2. **No file versioning**: Source files updated in place without version history
3. **PostgreSQL overhead**: Requires server maintenance and API infrastructure
4. **Disconnected workflows**: No systematic connection between ingest and publish steps
5. **Manual sequencing**: Users must know to run bottle ingestion before larvae ingestion

---

## File Versioning Strategy

### The Two-World Problem

CalCOFI data originates from multiple sources managed by different individuals:

- **Rasmus**: Bottle/cast data, CTD profiles
- **Ed**: Ichthyoplankton (larvae, eggs) data
- **Nastassia**: eDNA data
- **Linsey**: Zooplankton samples

These dataset managers need a simple interface (Google Drive) to deposit and organize files, while the data engineering side needs version control, immutability, and reproducibility.

### Google Drive: The Human Interface

This is "**The Drive**" from @sec-overview. Dataset managers upload files here; everything else is automatic.

#### Folder Structure

**Recommended organization**: `{provider}/{dataset}/raw/` and `{provider}/{dataset}/derived/`

```
calcofi/data-public/                       # Shared Google Drive folder (public data)
‚îú‚îÄ‚îÄ calcofi.org/
‚îÇ   ‚îî‚îÄ‚îÄ bottle-database/
‚îÇ       ‚îú‚îÄ‚îÄ raw/                           # Original exports from source system
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ bottle_export_2026-01.csv
‚îÇ       ‚îî‚îÄ‚îÄ derived/                       # Cleaned/processed versions
‚îÇ           ‚îú‚îÄ‚îÄ bottle.csv
‚îÇ           ‚îú‚îÄ‚îÄ cast.csv
‚îÇ           ‚îî‚îÄ‚îÄ field_descriptions.csv
‚îú‚îÄ‚îÄ swfsc.noaa.gov/
‚îÇ   ‚îî‚îÄ‚îÄ calcofi-db/
‚îÇ       ‚îú‚îÄ‚îÄ raw/                           # Original NOAA database exports
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ calcofi_db_dump_2026-01.sql
‚îÇ       ‚îî‚îÄ‚îÄ derived/                       # Extracted CSV tables
‚îÇ           ‚îú‚îÄ‚îÄ cruise.csv
‚îÇ           ‚îú‚îÄ‚îÄ egg.csv
‚îÇ           ‚îú‚îÄ‚îÄ larva.csv
‚îÇ           ‚îú‚îÄ‚îÄ net.csv
‚îÇ           ‚îú‚îÄ‚îÄ ship.csv
‚îÇ           ‚îú‚îÄ‚îÄ site.csv
‚îÇ           ‚îú‚îÄ‚îÄ species.csv
‚îÇ           ‚îî‚îÄ‚îÄ tow.csv
‚îú‚îÄ‚îÄ coastwatch.pfeg.noaa.gov/
‚îÇ   ‚îî‚îÄ‚îÄ erdCalCOFIlrvsiz/
‚îÇ       ‚îî‚îÄ‚îÄ derived/
‚îÇ           ‚îî‚îÄ‚îÄ larvae_size.csv
‚îî‚îÄ‚îÄ _archive/                              # Manual archive (optional)
    ‚îî‚îÄ‚îÄ 2025-01-15_bottle.csv              # User-created backup

calcofi/data-private/                      # Private/sensitive data (restricted access)
‚îú‚îÄ‚îÄ edna/                                  # eDNA samples (Nastassia)
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ derived/
‚îú‚îÄ‚îÄ zooplankton/                           # Zooplankton samples (Linsey)
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ derived/
‚îî‚îÄ‚îÄ _archive/                              # Manual archive (optional)
```

#### Google Drive Naming Conventions

- **Top-level folders**: `{provider}/` - the organization providing the data (e.g., `swfsc.noaa.gov`)
- **Dataset folders**: `{dataset}/` - specific dataset name (e.g., `calcofi-db`)
- **Subfolders** (recommended):
  - `raw/` - Original data as received from source (database dumps, original CSVs)
  - `derived/` - Processed/cleaned versions ready for ingestion
- **Files**: Original source names, no date prefixes required
- **Updates**: Simply overwrite existing files
- **No versioning in Drive**: Google Drive does not maintain file history for CSVs

#### Dataset Manager Workflow

1. Create folder structure: `{provider}/{dataset}/raw/` and `{provider}/{dataset}/derived/`
2. Upload original data to `raw/` (preserve original format)
3. Upload processed/cleaned data to `derived/` (CSV format preferred)
4. Simply overwrite files when updating‚Äîversioning happens automatically in the Archive
5. Optionally notify data team of significant changes

**See @sec-overview for the big picture of how your data flows through the system.**

### Google Cloud Storage: The Versioned Data Lake

This is "**The Archive**" from @sec-overview[^datalake-detail]. It automatically captures versioned snapshots of all files from the Drive.

[^datalake-detail]: Technical terms: This is a "versioned data lake" implemented with Google Cloud Storage buckets and rclone synchronization with timestamped archive snapshots.

#### Bucket Structure

```
gs://calcofi-files-public/                 # Versioned public source files
‚îú‚îÄ‚îÄ _sync/                                 # Working directory (rclone syncs here)
‚îÇ   ‚îú‚îÄ‚îÄ calcofi.org/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bottle-database/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ bottle.csv
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ cast.csv
‚îÇ   ‚îî‚îÄ‚îÄ swfsc.noaa.gov/
‚îÇ       ‚îî‚îÄ‚îÄ calcofi-db/
‚îÇ           ‚îú‚îÄ‚îÄ cruise.csv
‚îÇ           ‚îú‚îÄ‚îÄ egg.csv
‚îÇ           ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ archive/                               # Timestamped immutable snapshots
‚îÇ   ‚îú‚îÄ‚îÄ 2026-02-02_121557/                # YYYY-MM-DD_HHMMSS
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ calcofi.org/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ bottle-database/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ bottle.csv            # Snapshot at that time
‚îÇ   ‚îú‚îÄ‚îÄ 2026-02-15_120000/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ swfsc.noaa.gov/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ calcofi-db/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ larva.csv
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ manifests/                             # Version metadata
    ‚îú‚îÄ‚îÄ manifest_2026-02-02_121557.json
    ‚îú‚îÄ‚îÄ manifest_2026-02-15_120000.json
    ‚îî‚îÄ‚îÄ manifest_latest.json              # Points to latest archive

gs://calcofi-files-private/                # Versioned private source files
‚îú‚îÄ‚îÄ _sync/                                 # Working directory (rclone syncs here)
‚îÇ   ‚îú‚îÄ‚îÄ edna/
‚îÇ   ‚îî‚îÄ‚îÄ zooplankton/
‚îú‚îÄ‚îÄ archive/                               # Timestamped immutable snapshots
‚îî‚îÄ‚îÄ manifests/

gs://calcofi-db/                           # Integrated database
‚îú‚îÄ‚îÄ parquet/                               # Transformed tables
‚îÇ   ‚îú‚îÄ‚îÄ bottle.parquet
‚îÇ   ‚îú‚îÄ‚îÄ cast.parquet
‚îÇ   ‚îú‚îÄ‚îÄ larvae.parquet
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ duckdb/                                # DuckDB database files
‚îÇ   ‚îî‚îÄ‚îÄ calcofi.duckdb
‚îî‚îÄ‚îÄ ducklake/                              # DuckLake catalog
    ‚îî‚îÄ‚îÄ catalog.json
```

#### GCS Naming Conventions

- **_sync/**: Working directory, exact mirror of Google Drive structure (rclone syncs here)
- **archive/{YYYY-MM-DD_HHMMSS}/**: Immutable snapshot of complete data at that timestamp
- **manifests/manifest_{YYYY-MM-DD_HHMMSS}.json**: Snapshot metadata
- **manifests/manifest_latest.json**: Points to latest archive snapshot

### Rclone Sync: Capturing Daily Changes

#### Installation

**macOS:**

```bash
brew install rclone
```

**Linux (Ubuntu/Debian):**

```bash
curl https://rclone.org/install.sh | sudo bash
```

#### Configuration

Run `rclone config` and create two remotes:

**Remote 1: Google Drive (`gdrive`)**

```
n) New remote
name> gdrive
Storage> drive
client_id>                        # leave blank
client_secret>                    # leave blank
scope> 1                          # Full access (or 2 for read-only)
service_account_file>             # leave blank for OAuth, or path to JSON key
Edit advanced config> n
Use auto config> y                # opens browser (use 'n' on headless server)
Configure this as a Shared Drive> n
```

<!---
Ben's laptop output on 2026-02-01:

Configuration complete.
Options:
- type: drive
- scope: drive
- token: {"access_token":"...","expiry":"2026-02-01T01:40:32.653983-06:00","expires_in":3599}
- team_drive: 
Keep this "gdrive-ecoquants" remote?
y) Yes this is OK (default)
e) Edit this remote
d) Delete this remote
y/e/d> y

Current remotes:

Name                 Type
====                 ====
gcs-calcofi          google cloud storage
gdrive-ecoquants     drive
-->


**Remote 2: Google Cloud Storage (`gcs`)**

```
n) New remote
name> gcs
Storage> google cloud storage
client_id>                        # leave blank
client_secret>                    # leave blank
project_number> ucsd-sio-calcofi
service_account_file>             # leave blank for OAuth, or path to JSON key
Edit advanced config> n
Use auto config> y                # opens browser (use 'n' on headless server)
```

<!--
Ben's laptop output on 2026-02-01:

Use web browser to automatically authenticate rclone with remote?
 * Say Y if the machine running rclone has a web browser you can use
 * Say N if running rclone on a (remote) machine without web browser access
If not sure try Y. If Y failed, try N.

y) Yes (default)
n) No
y/n> 

2026/02/01 00:29:35 NOTICE: Make sure your Redirect URL is set to "http://127.0.0.1:53682/" in your custom config.
2026/02/01 00:29:35 NOTICE: If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=paZTK8N-GsB5j3kJDEq5SA
2026/02/01 00:29:35 NOTICE: Log in and authorize rclone for access
2026/02/01 00:29:35 NOTICE: Waiting for code...
2026/02/01 00:29:48 NOTICE: Got code
Configuration complete.
Options:
- type: google cloud storage
- project_number: ucsd-sio-calcofi
- object_acl: publicRead
- bucket_acl: publicRead
- bucket_policy_only: true
- token: {"access_token":"...","expiry":"2026-02-01T01:29:48.100866-06:00","expires_in":3599}
Keep this "gcs-calcofi" remote?
y) Yes this is OK (default)
e) Edit this remote
d) Delete this remote
y/e/d> y

Current remotes:

Name                 Type
====                 ====
gcs-calcofi          google cloud storage
gdrive               drive
-->

#### Headless Server Setup

For Linux servers without a browser, use `rclone authorize` on a local machine:

```bash
# on local machine with browser
rclone authorize "drive"
# copy the token output

# on server, during rclone config
Use auto config> n
# paste the token when prompted
```

Similarly for GCS:

```bash
# on local machine
rclone authorize "google cloud storage"
```

#### Service Account (Optional)

For automated/scheduled syncs, use a GCP service account instead of OAuth:

1. Create service account in GCP Console ‚Üí IAM ‚Üí Service Accounts
2. Grant roles: `Storage Object Admin` for `calcofi-files-public`, `calcofi-files-private`, and `calcofi-db` buckets
3. Download JSON key file to secure location (e.g., `/etc/rclone/calcofi-sa.json`)
4. In `rclone config`, provide path to JSON key instead of using OAuth

#### Verify Configuration

```bash
# test Google Drive access
rclone lsd gdrive:projects/calcofi/data-public      # OR
rclone lsd gdrive-ecoquants:projects/calcofi/data-public

# test GCS access
rclone ls gcs:calcofi-files-public --max-depth 1    # OR
rclone ls gcs-calcofi:calcofi-files-public --max-depth 1

# test sync (dry-run first!)
rclone sync gdrive:calcofi/data-public gcs:calcofi-files-public/_sync --dry-run -v
rclone sync gdrive-ecoquants:projects/calcofi/data-public gcs-calcofi:calcofi-files-public/_sync --dry-run -v
```

#### How It Works

```bash
#!/bin/bash
# sync_gdrive_to_gcs.sh - Run daily via cron

TIMESTAMP=$(date +%Y-%m-%d_%H%M%S)
LOG_FILE="/var/log/rclone/sync_${TIMESTAMP}.log"

# --- PUBLIC DATA ---
# Step 1: Sync Google Drive to _sync/ working directory
rclone sync gdrive:calcofi/data-public gs://calcofi-files-public/_sync \
  --checksum \
  --drive-export-formats csv \
  --log-file ${LOG_FILE} \
  --log-level INFO

# Step 2: Create immutable archive snapshot
rclone copy gs://calcofi-files-public/_sync gs://calcofi-files-public/archive/${TIMESTAMP}

# Step 3: Generate and upload manifest
rclone lsjson gs://calcofi-files-public/archive/${TIMESTAMP} --recursive \
  > /tmp/manifest_${TIMESTAMP}.json
rclone copy /tmp/manifest_${TIMESTAMP}.json gs://calcofi-files-public/manifests/
rclone copyto /tmp/manifest_${TIMESTAMP}.json gs://calcofi-files-public/manifests/manifest_latest.json

# --- PRIVATE DATA ---
# Repeat for private bucket (separate log file)
rclone sync gdrive:calcofi/data-private gs://calcofi-files-private/_sync \
  --checksum \
  --drive-export-formats csv \
  --log-file ${LOG_FILE%.log}_private.log \
  --log-level INFO

rclone copy gs://calcofi-files-private/_sync gs://calcofi-files-private/archive/${TIMESTAMP}
```

#### Key Behaviors

| Scenario | Google Drive Action | Rclone Behavior | Result in GCS |
|----------|--------------------|-----------------|----|
| New file | User uploads `new.csv` | `rclone sync` copies to `_sync/` | `_sync/new.csv` created, next archive includes it |
| Updated file | User overwrites `bottle.csv` | `rclone sync` updates `_sync/` | `_sync/bottle.csv` updated, archive snapshots preserve history |
| Deleted file | User deletes `old.csv` | `rclone sync` removes from `_sync/` | Removed from `_sync/`, preserved in prior archives |
| Renamed file | User renames `a.csv` ‚Üí `b.csv` | Treated as delete + create | `a.csv` gone from `_sync/`, `b.csv` created |
| No changes | Nothing | `--checksum` skips identical files | No new sync needed |

#### The Two-Step Archive Strategy

The archiving approach uses two steps for reliability:

1. **Sync to `_sync/`**: rclone syncs Google Drive to the `_sync/` working directory
2. **Copy to `archive/{timestamp}/`**: Create an immutable snapshot of the current state

Benefits:
- **Immutable archives**: Each archive folder is a complete snapshot, not just deltas
- **Reproducibility**: Any archive folder contains everything needed to recreate that state
- **No data loss**: If sync fails, previous archives remain intact
- **Public access**: `calcofi-files-public` bucket has public read access for easy data sharing

### Manifest Files

Each manifest captures the complete state of the archive at a point in time:

```json
{
  "generated_at": "2026-02-02T12:15:57Z",
  "archive_timestamp": "2026-02-02_121557",
  "bucket": "gs://calcofi-files-public",
  "archive_path": "archive/2026-02-02_121557",
  "files": [
    {
      "Path": "calcofi.org/bottle-database/bottle.csv",
      "Size": 45234567,
      "ModTime": "2026-02-02T10:30:00Z",
      "MimeType": "text/csv",
      "gcs_url": "gs://calcofi-files-public/archive/2026-02-02_121557/calcofi.org/bottle-database/bottle.csv"
    },
    {
      "Path": "swfsc.noaa.gov/calcofi-db/larva.csv",
      "Size": 12345678,
      "ModTime": "2026-02-01T08:00:00Z",
      "MimeType": "text/csv",
      "gcs_url": "gs://calcofi-files-public/archive/2026-02-02_121557/swfsc.noaa.gov/calcofi-db/larva.csv"
    }
  ],
  "total_files": 112457,
  "total_size_bytes": 35987654321
}
```

### Reconstructing Historical State

To recreate the data lake as it existed on a specific date:

```r
# R function to get file from a specific archive snapshot
get_archived_file <- function(path, timestamp = "latest", bucket = "calcofi-files-public") {
  # Get manifest for that timestamp
  if (timestamp == "latest") {
    manifest_url <- glue("gs://{bucket}/manifests/manifest_latest.json")
  } else {
    manifest_url <- glue("gs://{bucket}/manifests/manifest_{timestamp}.json")
  }

  manifest <- jsonlite::read_json(manifest_url)

  # Construct the archive URL
  archive_url <- glue("gs://{bucket}/archive/{manifest$archive_timestamp}/{path}")

  # Or use public HTTPS URL for direct access
  https_url <- glue("https://storage.googleapis.com/{bucket}/archive/{manifest$archive_timestamp}/{path}")

  return(https_url)
}
```

### Automation Schedule

```{mermaid}
%%| label: fig-schedule
%%| fig-cap: "Daily Sync Schedule (UTC)"

gantt
    title Daily Sync Schedule (UTC)
    dateFormat HH:mm
    axisFormat %H:%M

    section Sync
    rclone sync & archive     :a1, 00:00, 15m
    Generate manifest         :a2, after a1, 5m

    section Pipeline
    targets pipeline          :b1, 00:15, 15m
    CSV ‚Üí Parquet            :b2, after b1, 5m
    DuckDB update            :b3, after b2, 5m

    section Notify
    Send notifications        :c1, 00:30, 5m
```

---

## Proposed Architecture

### Data Flow Diagram

```{mermaid}
%%| label: fig-data-flow
%%| fig-cap: "CalCOFI Data Flow Architecture"

flowchart TB
    subgraph contributors["DATA CONTRIBUTOR LAYER"]
        gdrive_pub["Google Drive<br/>(calcofi/data-public/)"]
        gdrive_priv["Google Drive<br/>(calcofi/data-private/)"]
        csv1["calcofi.org/bottle-database/*.csv"]
        csv2["swfsc.noaa.gov/calcofi-db/*.csv"]
        csv3["coastwatch.pfeg.noaa.gov/*.csv"]
        csv4["edna/, zooplankton/"]
        gdrive_pub --- csv1
        gdrive_pub --- csv2
        gdrive_pub --- csv3
        gdrive_priv --- csv4
    end

    subgraph gcs["VERSIONED DATA LAKE (GCS)<br/>Project: ucsd-sio-calcofi"]
        bucket_pub["gs://calcofi-files-public/"]
        bucket_priv["gs://calcofi-files-private/"]
        sync_pub["_sync/"]
        archive_pub["archive/"]
        manifests_pub["manifests/"]
        bucket_pub --- sync_pub
        bucket_pub --- archive_pub
        bucket_pub --- manifests_pub
        bucket_priv --- sync_priv["_sync/"]
        bucket_priv --- archive_priv["archive/"]
    end

    subgraph db["INTEGRATED DATABASE"]
        bucket2["gs://calcofi-db/"]
        parquet["parquet/"]
        duckdb["duckdb/calcofi.duckdb"]
        ducklake["ducklake/"]
        tiles["tiles/*.pmtiles"]
        bucket2 --- parquet
        bucket2 --- duckdb
        bucket2 --- ducklake
        bucket2 --- tiles
    end

    subgraph publish["PUBLISHING LAYER"]
        obis["OBIS<br/>(DarwinCore Archive)"]
        edi["EDI<br/>(EML + data files)"]
        erddap["ERDDAP<br/>(NetCDF/CSV)"]
        odis["ODIS<br/>(schema.org JSON-LD)"]
    end

    gdrive_pub -->|"rclone sync"| bucket_pub
    gdrive_priv -->|"rclone sync"| bucket_priv
    gcs -->|"targets pipeline<br/>(CSV ‚Üí Parquet)"| db
    db -->|"publish workflows"| publish
```

### Technology Stack

| Layer | Technology | Purpose | Replaces |
|-------|------------|---------|----------|
| **Storage** | | | |
| | `gs://calcofi-files-public` | Versioned public source CSV files | Google Drive only |
| | `gs://calcofi-files-private` | Versioned private/sensitive files | Google Drive only |
| | `gs://calcofi-db` | Parquet, DuckDB, PMTiles | - |
| **Sync** | rclone | Drive ‚Üí GCS with archive snapshots | Manual copying |
| **Format** | Apache Parquet | Efficient columnar storage | CSV |
| **Compute** | DuckDB | Serverless SQL queries | PostgreSQL (gradual) |
| **Database** | | | |
| | Working DuckLake | Internal DB with provenance, time travel | PostgreSQL |
| | [Frozen DuckLake](https://ducklake.select/2025/10/24/frozen-ducklake/) | Public versioned releases (v2026.02, etc.) | PostgreSQL snapshots |
| **Orchestration** | `targets` | Dependency management, caching | Manual sequencing |
| **Tiles** | PMTiles + tippecanoe | Cloud-native vector tiles | pg_tileserv |
| **Mapping** | mapgl | Modern WebGL maps in Shiny | leaflet + pg_tileserv |
| **Functions** | `calcofi4r`, `calcofi4db` | Reusable R code | Ad-hoc scripts |

**GCP Project**: `ucsd-sio-calcofi`

---

## Database Architecture: Working vs Frozen DuckLake {#sec-database}

The CalCOFI integrated database uses [DuckLake](https://ducklake.select/), a lakehouse catalog that provides version control for data tables. We maintain **two distinct databases** to serve different needs:

### The Two-Database Strategy

```{mermaid}
%%| label: fig-ducklake-architecture
%%| fig-cap: "Working vs Frozen DuckLake Architecture"

flowchart TB
    subgraph archive["üìÅ Archive (GCS)"]
        csv["CSV source files"]
    end

    subgraph working["<b>üîß WORKING DUCKLAKE</b><br/><i>gs://calcofi-db/ducklake/working/</i>"]
        direction TB
        w_tables["Tables with provenance columns:<br/>‚Ä¢ _source_file<br/>‚Ä¢ _source_row<br/>‚Ä¢ _source_uuid<br/>‚Ä¢ _ingested_at"]
        w_history["Full edit history<br/>(time travel enabled)"]
        w_access["Access: Workflows only"]
    end

    subgraph frozen["<b>‚ùÑÔ∏è FROZEN DUCKLAKE</b><br/><i>gs://calcofi-db/ducklake/releases/</i>"]
        direction TB
        f_versions["Versioned releases:<br/>‚Ä¢ v2026.02<br/>‚Ä¢ v2026.03<br/>‚Ä¢ v2026.04"]
        f_clean["Clean tables<br/>(no provenance columns)"]
        f_access["Access: Public<br/>Apps, API, R package"]
    end

    subgraph outputs["üåê Public Access"]
        apps["Shiny Apps"]
        api["REST API"]
        rpkg["calcofi4r"]
        direct["Direct Parquet reads"]
    end

    csv -->|"ingest workflows"| working
    working -->|"freeze release"| frozen
    frozen --> outputs

    style archive fill:#e3f2fd,stroke:#1565c0
    style working fill:#fff3e0,stroke:#ef6c00
    style frozen fill:#e8f4e8,stroke:#2e7d32
    style outputs fill:#fce4ec,stroke:#c2185b
```

### Working DuckLake (Internal)

The Working DuckLake is the **internal, mutable database** used by workflows. It prioritizes:

- **Full provenance tracking**: Every row knows where it came from
- **Time travel**: Query the database as it existed at any past point
- **Frequent updates**: Can be modified whenever new data arrives

**Schema features**:

| Column | Type | Purpose |
|--------|------|---------|
| `_source_file` | VARCHAR | Path to original CSV file in Archive |
| `_source_row` | INTEGER | Row number in source file |
| `_source_uuid` | UUID | Original record ID (e.g., from NOAA database) |
| `_ingested_at` | TIMESTAMP | When this row was added/updated |

**Example query with provenance**:
```sql
-- Find all larvae records from a specific source file
SELECT scientific_name, count, _source_file, _source_row
FROM larvae
WHERE _source_file LIKE '%swfsc.noaa.gov/calcofi-db/larva.csv'
  AND cruise_id = '202301CC';
```

**Time travel query**:
```sql
-- Query larvae table as it existed on January 15th
SELECT * FROM larvae
AT TIMESTAMP '2026-01-15 00:00:00';
```

### Frozen DuckLake (Public Releases) {#sec-frozen}

The Frozen DuckLake provides **stable, versioned snapshots** for external users. Based on the [Frozen DuckLake](https://ducklake.select/2025/10/24/frozen-ducklake/) pattern:

**Key characteristics**:

- **Immutable**: Once released, a version never changes
- **Clean schema**: No internal provenance columns (cleaner for users)
- **Semantic versioning**: `v{YYYY}.{MM}` format (e.g., `v2026.02`)
- **Public access**: Anyone can query via HTTP/Parquet

**Release process**:

```{mermaid}
%%| label: fig-freeze-process
%%| fig-cap: "Freezing a Release"

flowchart LR
    working["Working DuckLake"]
    validate["Validate data quality"]
    strip["Remove provenance columns"]
    version["Tag version (v2026.02)"]
    upload["Upload to releases/"]
    announce["Announce release"]

    working --> validate --> strip --> version --> upload --> announce

    style working fill:#fff3e0,stroke:#ef6c00
    style validate fill:#fff9c4,stroke:#f9a825
    style strip fill:#fff9c4,stroke:#f9a825
    style version fill:#fff9c4,stroke:#f9a825
    style upload fill:#e8f4e8,stroke:#2e7d32
    style announce fill:#e8f4e8,stroke:#2e7d32
```

**Frozen release structure**:
```
gs://calcofi-db/ducklake/releases/
‚îú‚îÄ‚îÄ v2026.02/
‚îÇ   ‚îú‚îÄ‚îÄ catalog.json          # DuckLake catalog
‚îÇ   ‚îú‚îÄ‚îÄ parquet/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bottle.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cast.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ larvae.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ RELEASE_NOTES.md
‚îú‚îÄ‚îÄ v2026.03/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ latest -> v2026.03        # Symlink to latest
```

### Accessing Frozen Releases

**From R (using calcofi4r)**:
```r
library(calcofi4r)

# Get latest release (default)
con <- cc_get_db()

# Get specific version
con <- cc_get_db(version = "v2026.02")

# List available versions
cc_list_versions()
#> [1] "v2026.02" "v2026.03" "v2026.04"
```

**Direct Parquet access (no R required)**:
```python
import duckdb

# Query directly from GCS (no download needed)
con = duckdb.connect()
con.execute("""
    SELECT * FROM read_parquet(
        'https://storage.googleapis.com/calcofi-db/ducklake/releases/latest/parquet/larvae.parquet'
    )
    LIMIT 10
""")
```

**From any HTTP client**:
```bash
# Download parquet file directly
curl -O https://storage.googleapis.com/calcofi-db/ducklake/releases/v2026.02/parquet/larvae.parquet
```

### Release Schedule

| Frequency | Trigger | Example |
|-----------|---------|---------|
| **Monthly** | Regular data updates | v2026.02, v2026.03 |
| **Ad-hoc** | Major dataset additions | v2026.02.1 (patch) |
| **Annual** | Year-end comprehensive release | v2026.12 |

**Release checklist**:

- [ ] All ingest workflows pass
- [ ] Data quality checks pass (no nulls in required fields, valid ranges)
- [ ] Foreign key relationships valid
- [ ] Row counts within expected range
- [ ] Release notes document changes
- [ ] Announce to users

---

## R Package Strategy

### Package Roles

#### `calcofi4db` - Database Administration Tools
**Target users**: Data managers, workflow developers

```
calcofi4db/R/
‚îú‚îÄ‚îÄ cloud.R        # GCS operations (@concept cloud)
‚îú‚îÄ‚îÄ read.R         # CSV/Parquet reading (@concept read)
‚îú‚îÄ‚îÄ transform.R    # Data transformation (@concept transform)
‚îú‚îÄ‚îÄ ingest.R       # Database loading (@concept ingest)
‚îú‚îÄ‚îÄ parquet.R      # Parquet operations (@concept parquet)   [NEW]
‚îú‚îÄ‚îÄ duckdb.R       # DuckDB operations (@concept duckdb)     [NEW]
‚îú‚îÄ‚îÄ version.R      # Schema versioning (@concept version)
‚îú‚îÄ‚îÄ check.R        # Data validation (@concept check)
‚îî‚îÄ‚îÄ viz.R          # Diagram generation (@concept viz)
```

#### `calcofi4r` - User-Facing Tools
**Target users**: Researchers, analysts, app developers

```
calcofi4r/R/
‚îú‚îÄ‚îÄ read.R         # Data retrieval (@concept read)
‚îú‚îÄ‚îÄ database.R     # Database connection (@concept database)
‚îú‚îÄ‚îÄ analyze.R      # Statistical analysis (@concept analyze)
‚îú‚îÄ‚îÄ visualize.R    # Plotting and mapping (@concept visualize)
‚îî‚îÄ‚îÄ functions.R    # Helper functions (@concept utils)
```

### Function Reuse Across Contexts

| Context | Package | Example Usage |
|---------|---------|---------------|
| Ingestion workflows | `calcofi4db` | `read_csv_files()`, `ingest_csv_to_db()` |
| Publishing workflows | `calcofi4db` | `get_db_con()`, `create_db_manifest()` |
| Shiny apps | `calcofi4r` | `cc_get_db()`, `cc_plot_timeseries()` |
| Analysis scripts | `calcofi4r` | `cc_read_bottle()`, `cc_analyze_trend()` |
| API endpoints | `calcofi4r` | `cc_query_db()`, `cc_get_variables()` |

### New Functions to Implement

#### In `calcofi4db`:

```r
# R/cloud.R - GCS operations
get_gcs_file(bucket, path, local_path = NULL)
put_gcs_file(local_path, bucket, path)
sync_gdrive_to_gcs(gdrive_path, gcs_bucket, backup = TRUE)
list_gcs_versions(bucket, path)

# R/parquet.R - Parquet operations
csv_to_parquet(csv_path, schema_def = NULL)
read_parquet_table(path, con = NULL)
write_parquet_table(data, path, partitions = NULL)
add_parquet_metadata(path, metadata_list)

# R/duckdb.R - DuckDB operations
get_duckdb_con(path = ":memory:", read_only = FALSE)
create_duckdb_views(con, manifest)
attach_ducklake(con, catalog_path)
set_duckdb_comments(con, table, comments)

# R/ducklake.R - DuckLake catalog operations [NEW]
get_working_ducklake()           # connect to internal Working DuckLake
ingest_to_working(data, table, source_file, source_uuid_col = NULL)
add_provenance_columns(data, source_file, source_row_start = 1)

# R/freeze.R - Frozen release operations [NEW]
freeze_release(version, release_notes = NULL)
  # 1. Validate data quality

  # 2. Strip provenance columns (_source_*, _ingested_at)
  # 3. Export to parquet files
  # 4. Create DuckLake catalog
  # 5. Upload to releases/{version}/
validate_for_release(con)        # run all quality checks before freeze
list_frozen_releases()           # list available frozen versions
compare_releases(v1, v2)         # diff two releases
```

#### In `calcofi4r`:
```r
# R/database.R - Frozen DuckLake access (user-facing)
cc_get_db(version = "latest", local_cache = TRUE)
  # Connect to frozen release (never Working DuckLake)
cc_list_versions()               # list available frozen releases
cc_db_info(version = "latest")   # release date, row counts, notes
cc_release_notes(version)        # view release notes for a version

# R/read.R - Convenience functions for common tables
cc_read_bottle(version = "latest", ...)
cc_read_larvae(version = "latest", ...)
cc_read_cast(version = "latest", ...)

# R/query.R - Query helpers
cc_query(sql, version = "latest")
cc_list_tables(version = "latest")
cc_describe_table(table, version = "latest")
```

---

## Workflow Dependency Management with `targets`

### Pipeline Definition

Create `workflows/_targets.R`:

```r
library(targets)
library(tarchetypes)

tar_option_set(
  packages = c("calcofi4db", "duckdb", "dplyr", "arrow"),
  format = "qs"  # Fast serialization
)

# Define pipeline
list(
  # ‚îÄ‚îÄ‚îÄ Raw Data from GCS (public bucket, latest archive) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  tar_target(
    raw_bottle,
    get_gcs_file("gs://calcofi-files-public", "calcofi.org/bottle-database/bottle.csv")
  ),
  tar_target(
    raw_cast,
    get_gcs_file("gs://calcofi-files-public", "calcofi.org/bottle-database/cast.csv")
  ),
  tar_target(
    raw_larvae,
    get_gcs_file("gs://calcofi-files-public", "swfsc.noaa.gov/calcofi-db/larva.csv")
  ),

  # ‚îÄ‚îÄ‚îÄ Transform to Parquet ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  tar_target(
    pqt_bottle,
    csv_to_parquet(raw_bottle, output = "parquet/bottle.parquet")
  ),
  tar_target(
    pqt_cast,
    csv_to_parquet(raw_cast, output = "parquet/cast.parquet")
  ),
  tar_target(
    pqt_larvae,
    csv_to_parquet(raw_larvae, output = "parquet/larvae.parquet")
  ),

  # ‚îÄ‚îÄ‚îÄ Create Integrated DuckDB ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  tar_target(
    db_manifest,
    create_db_manifest(
      tables = list(pqt_bottle, pqt_cast, pqt_larvae),
      version = format(Sys.Date(), "%Y.%m.%d")
    )
  ),
  tar_target(
    duckdb_file,
    create_duckdb_from_manifest(db_manifest)
  ),

  # ‚îÄ‚îÄ‚îÄ Publish Datasets ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  tar_target(
    obis_archive,
    create_obis_archive(duckdb_file, include_bottle = TRUE)
  )
)
```

### Workflow Lifecycle

```{mermaid}
%%| label: fig-lifecycle
%%| fig-cap: "Development and Update Cycles"

flowchart TB
    subgraph dev["Development Cycle"]
        direction LR
        d1["1. Explore data<br/><code>explore_*.qmd</code>"]
        d2["2. Design ingest<br/><code>ingest_*.qmd</code>"]
        d3["3. Add to targets<br/><code>_targets.R</code>"]
        d4["4. Run pipeline<br/><code>tar_make()</code>"]
        d5["5. Publish<br/><code>publish_*.qmd</code>"]
        d1 --> d2 --> d3 --> d4 --> d5
    end

    subgraph update["Update Cycle"]
        direction LR
        u1["1. New data<br/>Google Drive"]
        u2["2. Sync to GCS<br/>rclone"]
        u3["3. Run pipeline<br/><code>tar_make()</code>"]
        u4["4. Republish<br/>automatic"]
        u1 --> u2 --> u3 --> u4
    end

    dev -.->|"formalized"| update
```

### Dependency Graph

```{mermaid}
%%| label: fig-targets
%%| fig-cap: "targets Pipeline Dependency Graph"

flowchart LR
    subgraph raw["Raw CSV"]
        raw_bottle
        raw_cast
        raw_larvae
    end

    subgraph parquet["Parquet"]
        pqt_bottle
        pqt_cast
        pqt_larvae
    end

    subgraph database["Database"]
        db_manifest
        duckdb_file
    end

    subgraph publish["Publish"]
        obis_archive
    end

    raw_bottle --> pqt_bottle
    raw_cast --> pqt_cast
    raw_larvae --> pqt_larvae

    pqt_bottle --> db_manifest
    pqt_cast --> db_manifest
    pqt_larvae --> db_manifest

    db_manifest --> duckdb_file
    duckdb_file --> obis_archive
```

---

## Documentation Updates

### Recommended Changes to `docs/`

#### `docs/db.qmd` - Database Documentation

**Current**: PostgreSQL-focused with naming conventions and ingestion strategy

**Proposed changes**:

- Add section on DuckDB as primary database
- Document Parquet file organization
- Explain DuckLake versioning and time travel
- Keep naming conventions (snake_case, *_id, *_uuid suffixes)
- Update "Integrated database ingestion strategy" for targets pipeline

#### `docs/api.qmd` - API Documentation

**Current**: REST API endpoints for PostgreSQL queries

**Proposed changes**:

- Note that REST API may be deprecated for most use cases
- Document direct DuckDB access via `calcofi4r::cc_get_db()`
- Keep API for web apps that cannot use DuckDB directly
- Consider lightweight API using DuckDB backend if needed

#### `docs/portals.qmd` - Portal Documentation

**Current**: Good overview of EDI, NCEI, OBIS, ERDDAP

**Proposed changes**:

- Update data flow diagram to show DuckDB ‚Üí Portals
- Add section on automated publishing via targets pipeline
- Document versioning strategy for portal submissions

### Where Documentation Should Live

| Content | Location | Rationale |
|---------|----------|-----------|
| Database schema | `docs/db.qmd` | User-facing documentation |
| Workflow development | `workflows/README.md` | Developer guide |
| Package functions | `calcofi4r/`, `calcofi4db/` | pkgdown reference |
| Data dictionary | `calcofi4db/inst/` | Versioned with code |
| Architecture decisions | `docs/` or `workflows/_docs/` | Long-term reference |

---

## Implementation Priority

### Phase 1: Foundation ‚úÖ COMPLETE (2026-02-02)

**Goal**: Establish cloud infrastructure and helper functions

1. **Set up GCS buckets** ‚úÖ
   - Created `gs://calcofi-files-public/` with public read access
   - Created `gs://calcofi-files-private/` with restricted access
   - Configured rclone access via OAuth

2. **Add cloud functions to `calcofi4db`** ‚úÖ
   - Created `R/cloud.R` with `get_gcs_file()`, `put_gcs_file()`
   - Created `R/parquet.R` with `csv_to_parquet()`
   - Created `R/duckdb.R` with `get_duckdb_con()`, `create_duckdb_views()`

3. **Initial sync** ‚úÖ
   - Synced Google Drive `data-public/` to `gs://calcofi-files-public/_sync/`
   - Created initial archive snapshot: `archive/2026-02-02_121557/`
   - Generated manifest: `manifests/manifest_2026-02-02_121557.json`
   - Total: 112,457 files (33.5 GiB)

### Phase 2: Pipeline (Weeks 3-6)

**Goal**: Implement targets pipeline for priority datasets

1. **Create `workflows/_targets.R`**
   - Define targets for bottle and larvae data
   - Establish Parquet transformation pipeline
   - Create integrated DuckDB

2. **Migrate ingestion workflows**
   - Convert `ingest_calcofi.org_bottle-database.qmd` to target
   - Convert `ingest_swfsc.noaa.gov_calcofi-db.qmd` to target

3. **Update publish workflow**
   - Modify `publish_larvae_to_obis.qmd` to read from DuckDB
   - Add bottle data to EMoF extension

### Phase 3: Working DuckLake (Weeks 7-10)

**Goal**: Implement Working DuckLake with provenance tracking

1. **Set up Working DuckLake catalog**
   - Configure DuckLake at `gs://calcofi-db/ducklake/working/`
   - Enable time travel queries
   - Create Git-tracked catalog manifest

2. **Add provenance columns to ingest workflows**
   - `_source_file`: Path to source CSV in Archive
   - `_source_row`: Row number in source file
   - `_source_uuid`: Original record UUID (if available)
   - `_ingested_at`: Ingestion timestamp

3. **Implement `calcofi4db` DuckLake functions**
   - `get_working_ducklake()`: Connect to internal database
   - `ingest_to_working()`: Ingest with provenance
   - `add_provenance_columns()`: Helper for provenance

### Phase 4: Frozen DuckLake (Weeks 11-14)

**Goal**: Implement [Frozen DuckLake](https://ducklake.select/2025/10/24/frozen-ducklake/) for public releases

1. **Create freeze workflow**
   - `validate_for_release()`: Data quality checks
   - `freeze_release()`: Create immutable release
   - Strip provenance columns for clean public schema
   - Generate release notes

2. **Set up release structure**
   - `gs://calcofi-db/ducklake/releases/v2026.02/`
   - Parquet files + DuckLake catalog
   - `latest` symlink to current release

3. **Update `calcofi4r` for frozen access**
   - `cc_get_db(version = "latest")`: Default to frozen releases
   - `cc_list_versions()`: List available releases
   - `cc_release_notes()`: View release notes

4. **First public release: v2026.02**
   - Bottle + Cast + Larvae tables
   - Announce to users

### Phase 5: Documentation & Polish (Weeks 15-16)

**Goal**: Complete documentation and evaluate API

1. **Update docs/**
   - Revise `db.qmd` for DuckDB/DuckLake
   - Document Working vs Frozen databases
   - Evaluate API necessity
   - Update portal documentation

2. **Clean up workflows/**
   - Consolidate README.md with essentials
   - Archive deprecated exploration notebooks
   - Document workflow development process
   - Create "Getting Started" guide for data managers

---

## Verification Plan

### End-to-End Test

```r
# 1. Sync test data from Google Drive to GCS
system("rclone sync gdrive:calcofi/data-public gs://calcofi-files-public/_sync")

# 2. Create archive snapshot
timestamp <- format(Sys.time(), "%Y-%m-%d_%H%M%S")
system(glue::glue("rclone copy gs://calcofi-files-public/_sync gs://calcofi-files-public/archive/{timestamp}"))

# 3. Run targets pipeline
targets::tar_make()

# 4. Verify database
con <- calcofi4db::get_duckdb_con("calcofi.duckdb")
DBI::dbListTables(con)
DBI::dbGetQuery(con, "SELECT COUNT(*) FROM larvae")

# 5. Verify OBIS archive
obistools::check_fields(read_csv("data/darwincore/larvae/occurrence.csv"))
```

### Integration Tests

- [ ] GCS upload/download roundtrip
- [ ] Parquet schema preservation
- [ ] DuckDB query performance vs PostgreSQL
- [ ] OBIS archive validation
- [ ] Time travel query accuracy

---

## Migration Strategy: PostgreSQL ‚Üí DuckDB

### Current PostgreSQL Dependencies

Based on `server/docker-compose.yml`, the following services depend on PostgreSQL:

| Service | Purpose | Migration Path |
|---------|---------|----------------|
| `postgis` | Spatial database | Keep for vector tiles (short-term) |
| `pg_tileserv` | Vector tile server | Replace with PMTiles |
| `pg_rest` | REST API (PostgREST) | Keep for web apps, add DuckDB option |
| `plumber` | R API | Update to use DuckDB |
| `pgadmin` | Database admin | Deprecate once migrated |

### Vector Tiles: pg_tileserv ‚Üí PMTiles

The current architecture uses `pg_tileserv` to serve vector tiles from PostGIS for Shiny mapping apps. The migration path:

```{mermaid}
%%| label: fig-tiles-migration
%%| fig-cap: "Vector Tiles Migration Path"

flowchart LR
    subgraph current["Current Flow"]
        direction LR
        pg["PostGIS<br/>(geometries)"]
        ts["pg_tileserv"]
        lf["Leaflet/Mapbox<br/>in Shiny"]
        pg --> ts --> lf
    end

    subgraph target["Target Flow"]
        direction LR
        geo["GeoParquet/<br/>GeoJSON"]
        tc["tippecanoe"]
        pm["PMTiles"]
        mg["mapgl::<br/>add_pmtiles_source()"]
        geo --> tc --> pm --> mg
    end

    current -.->|"migrate"| target
```

#### Implementation Steps

1. **Generate PMTiles from PostGIS**
   ```bash
   # Export geometries from PostGIS
   ogr2ogr -f GeoJSON stations.geojson \
     PG:"host=postgis user=admin dbname=gis" \
     -sql "SELECT * FROM stations"

   # Convert to PMTiles using tippecanoe
   tippecanoe -o stations.pmtiles \
     --minimum-zoom=0 --maximum-zoom=14 \
     --layer=stations \
     stations.geojson

   # Upload to GCS
   gsutil cp stations.pmtiles gs://calcofi-db/tiles/
   ```

2. **Update Shiny Apps**
   ```r
   # Old: pg_tileserv
   leaflet() |>
     addTiles() |>
     addMapboxGL(
       style = list(
         sources = list(
           stations = list(
             type = "vector",
             tiles = list("https://tile.calcofi.io/public.stations/{z}/{x}/{y}.pbf")
           )
         )
       )
     )

   # New: PMTiles with mapgl
   library(mapgl)
   mapboxgl() |>
     add_pmtiles_source(
       id = "stations",
       url = "https://storage.googleapis.com/calcofi-db/tiles/stations.pmtiles"
     ) |>
     add_layer(
       id = "stations-layer",
       type = "circle",
       source = "stations",
       source_layer = "stations"
     )
   ```

3. **Add PMTiles generation to targets pipeline**
   ```r
   # In _targets.R
   tar_target(
     pmtiles_stations,
     create_pmtiles(
       geom_source = duckdb_file,
       query = "SELECT * FROM stations",
       output = "tiles/stations.pmtiles"
     )
   )
   ```

### Migration Roadmap

| Phase | Timeline | Actions |
|-------|----------|---------|
| **Phase 1** | Now | Add DuckDB as primary data access; keep PostgreSQL |
| **Phase 2** | +2 months | Generate PMTiles for all spatial layers |
| **Phase 3** | +4 months | Update Shiny apps to use mapgl + PMTiles |
| **Phase 4** | +6 months | Deprecate pg_tileserv, reduce PostgreSQL to backup |
| **Phase 5** | +12 months | Fully deprecate PostgreSQL (if no dependencies remain) |

---

## Resolved Questions

Based on clarifications received:

1. **GCS Project**: Use existing project `ucsd-sio-calcofi`
   - Existing bucket `calcofi-db` ‚Üí DuckLake/database files
   - New bucket `calcofi-files-public` ‚Üí Versioned public source files (public read access)
   - New bucket `calcofi-files-private` ‚Üí Versioned private/sensitive files (restricted access)

2. **Google Drive Structure** (implemented 2026-02-02):
   - `calcofi/data-public/` ‚Üí Public datasets (bottle, larvae, etc.)
   - `calcofi/data-private/` ‚Üí Private/sensitive data (eDNA, zooplankton samples)

3. **GCS Folder Structure** (implemented 2026-02-02):
   - `_sync/` ‚Üí Working directory (rclone syncs here, mutable)
   - `archive/{timestamp}/` ‚Üí Immutable snapshots (complete copies, not deltas)
   - `manifests/` ‚Üí JSON metadata files with `manifest_latest.json` pointer

4. **API Strategy**: Keep PostgreSQL + API for now with migration roadmap
   - pg_tileserv needed for current Shiny apps
   - Migrate to PMTiles over time using tippecanoe + mapgl

5. **Priority Data**: Larvae + Bottle datasets for proof-of-concept

6. **Bottle in OBIS**: Include as EMoF extension in larvae dataset

## Remaining Questions

1. **DuckLake vs MotherDuck**: Self-hosted DuckLake catalog or use MotherDuck service?
2. **Automation**: GitHub Actions vs server cron for scheduled rclone syncs?
3. **PMTiles hosting**: Serve from GCS directly or via CDN?

---

## Appendix: Key References

### Documentation

- [Versioned Data Lake Strategy](workflows/_docs/2026-01-02%20Versioned%20Data%20Lake%20Strategy.md)
- [Data Management Action Plan](workflows/_docs/2026-01-27%20updated%20working%20draft_Data_Management_Action_Plan_CalCOFI.md)
- [Data Pipeline Timeline](workflows/_docs/2026-01-31%20Data%20Pipeline%20Timeline%20.md)

### Technologies

**Database & Analytics**:

- [DuckDB](https://duckdb.org/) - Fast in-process analytical database
- [DuckLake](https://ducklake.select/) - Lakehouse catalog for DuckDB
  - [Frozen DuckLake](https://ducklake.select/2025/10/24/frozen-ducklake/) - Immutable versioned releases
- [Apache Parquet](https://parquet.apache.org/) - Columnar storage format

**Workflow & Orchestration**:

- [targets R package](https://books.ropensci.org/targets/) - Pipeline dependency management

**Cloud Storage**:

- [rclone](https://rclone.org/) - Cloud storage sync tool
- [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/) - R interface to GCS
- [Data lakes and big data analytics - Google Cloud Storage](https://cloud.google.com/storage#data-lakes-and-big-data-analytics)

**Vector Tiles**:

- [PMTiles](https://protomaps.com/docs/pmtiles) - Cloud-native vector tile format
- [tippecanoe](https://github.com/felt/tippecanoe) - Build vector tilesets
- [mapgl R package](https://walker-data.com/mapgl/) - Modern WebGL maps in R/Shiny

### CalCOFI Resources

- [calcofi.io/docs](https://calcofi.io/docs) - CalCOFI documentation site
- [calcofi.io/calcofi4r](https://calcofi.io/calcofi4r) - User-facing R package
- [calcofi.io/calcofi4db](https://calcofi.io/calcofi4db) - Database admin R package
